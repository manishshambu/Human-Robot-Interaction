{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division, unicode_literals\n",
    "\n",
    "# Imports the Google Cloud client library, use for speech-to-text\n",
    "from google.cloud import speech\n",
    "from google.cloud.speech import enums\n",
    "from google.cloud.speech import types\n",
    "\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"/Users/anuj/coursework_cuboulder/spring_2018/algo_hri/torchMoji-master\")\n",
    "\n",
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# used for emotion feature extraction\n",
    "from torchmoji.sentence_tokenizer import SentenceTokenizer\n",
    "from torchmoji.model_def import torchmoji_emojis\n",
    "from torchmoji.global_variables import PRETRAINED_PATH, VOCAB_PATH\n",
    "\n",
    "import subprocess\n",
    "\n",
    "# used for audio preprocessing\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "from pydub.silence import detect_nonsilent, detect_silence\n",
    "\n",
    "import pickle\n",
    "\n",
    "# used for training the machine learning models\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import neighbors, linear_model\n",
    "\n",
    "# used for training LSTM model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras import backend as K\n",
    "\n",
    "import time\n",
    "\n",
    "from bodylang_utils import convert_audio_to_mono, get_relative_joint_positions, get_prosody_features_from_audio, get_text_from_speech, get_emotion_features_from_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASE POSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 15, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_poses_folder = '/Users/anuj/coursework_cuboulder/spring_2018/algo_hri/base_poses/'\n",
    "\n",
    "files = os.listdir(base_poses_folder)[1:]\n",
    "if 'base_poses_relative_joint_positions.json' not in files:\n",
    "    pose_ids = sorted(list(map(int, os.listdir(base_poses_folder)[1:])))\n",
    "    base_poses_relative_joint_positions = []\n",
    "    for pose_id in pose_ids:\n",
    "        folder = base_poses_folder + str(pose_id) + '/'\n",
    "        relative_joint_pos = get_relative_joint_positions(folder)[5]  # capture was stationary so pick a random frame, here 6 -- ideally, participant is 'settled' by then\n",
    "        del relative_joint_pos['time_ms']\n",
    "        base_poses_relative_joint_positions.append(relative_joint_pos)\n",
    "\n",
    "    with open(base_poses_folder+'base_poses_relative_joint_positions.json', 'w') as f:\n",
    "        json.dump(base_poses_relative_joint_positions, f)\n",
    "else:\n",
    "    base_poses_relative_joint_positions = json.load(open(base_poses_folder+'base_poses_relative_joint_positions.json'))\n",
    "\n",
    "num_base_poses = len(base_poses_relative_joint_positions)\n",
    "\n",
    "# convert base poses dictionary to 10x15x3 np.ndarray -- 10 poses, 15 joints, 3 position values x y z\n",
    "final_poses = []\n",
    "for base_pose_idx in range(num_base_poses):\n",
    "    p = []\n",
    "    final_pose = base_poses_relative_joint_positions[base_pose_idx]\n",
    "    for joint_name, joint_pos in final_pose.items():\n",
    "        p.append(joint_pos)\n",
    "    final_poses.append(p)\n",
    "final_poses = np.array(final_poses)\n",
    "\n",
    "final_poses.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/anuj/coursework_cuboulder/spring_2018/algo_hri/kinect_data/data0/\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anuj/anaconda/lib/python3.6/site-packages/torch/nn/modules/container.py:67: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1494, 64) (1494, 13) (1494, 128) (1494, 10)\n",
      "/Users/anuj/coursework_cuboulder/spring_2018/algo_hri/kinect_data/data1/\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anuj/anaconda/lib/python3.6/site-packages/torch/nn/modules/container.py:67: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(798, 64) (798, 13) (798, 128) (798, 10)\n",
      "/Users/anuj/coursework_cuboulder/spring_2018/algo_hri/kinect_data/data2/\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anuj/anaconda/lib/python3.6/site-packages/torch/nn/modules/container.py:67: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1655, 64) (1655, 13) (1655, 128) (1655, 10)\n",
      "/Users/anuj/coursework_cuboulder/spring_2018/algo_hri/kinect_data/data3/\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anuj/anaconda/lib/python3.6/site-packages/torch/nn/modules/container.py:67: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3790, 64) (3790, 13) (3790, 128) (3790, 10)\n",
      "/Users/anuj/coursework_cuboulder/spring_2018/algo_hri/kinect_data/data4/\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anuj/anaconda/lib/python3.6/site-packages/torch/nn/modules/container.py:67: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3057, 64) (3057, 13) (3057, 128) (3057, 10)\n",
      "/Users/anuj/coursework_cuboulder/spring_2018/algo_hri/kinect_data/data5/\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anuj/anaconda/lib/python3.6/site-packages/torch/nn/modules/container.py:67: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1253, 64) (1253, 13) (1253, 128) (1253, 10)\n",
      "(12047, 64) (12047, 13) (12047, 77) (12047, 10)\n"
     ]
    }
   ],
   "source": [
    "toplevel_folder = '/Users/anuj/coursework_cuboulder/spring_2018/algo_hri/kinect_data/'\n",
    "\n",
    "# np.ndarrays for feeding into the learning algorithm\n",
    "X_emotion = []\n",
    "X_prosody = []\n",
    "X_combined = []\n",
    "y_skeleton = []\n",
    "\n",
    "# for bookkeeping's sake, and later use\n",
    "combo = []\n",
    "pros = []\n",
    "em = []\n",
    "n_chunks = []\n",
    "l_chunks = []\n",
    "l_audio_chunks = []  # time in ms for each audio file, 1x7\n",
    "audio_portions_dicts = [] # for each audio portion, indicates whether to use pose 2 or run through NN\n",
    "auds = []  # nonsilent time portions for each audio file, idx indicates corresponce to l_chunks frames; to map feature vec ids\n",
    "\n",
    "for subfolder in sorted(os.listdir(toplevel_folder)[1:]):\n",
    "    data_folder = toplevel_folder + subfolder + '/'\n",
    "    print(data_folder)\n",
    "    print()\n",
    "\n",
    "    mono_audio_filename = convert_audio_to_mono(data_folder)\n",
    "\n",
    "    ###### get skeleton frames for each non-silent audio portion ######\n",
    "\n",
    "    skeleton_dict = get_relative_joint_positions(data_folder)\n",
    "\n",
    "    sound_file = AudioSegment.from_wav(mono_audio_filename)\n",
    "    l_audio_chunks.append(len(sound_file))\n",
    "    # TODO: could manually play around with these parameters to see what works best for each audio file\n",
    "    audio_chunks = split_on_silence(sound_file, \n",
    "        # must be silent for at least half a second>\n",
    "        min_silence_len=1000,\n",
    "\n",
    "        # consider it silent if quieter than -50 dBFS\n",
    "        silence_thresh=-50\n",
    "    )\n",
    "    \n",
    "    ###### get feature vectors for each non-silent audio portion #######\n",
    "\n",
    "    all_prosody_vectors = []\n",
    "    all_emoji_ids = []  # not used\n",
    "    all_emotion_vectors = []\n",
    "    nc=0\n",
    "\n",
    "    if not os.path.isdir(data_folder+'chunks/'):\n",
    "        os.mkdir(data_folder+'chunks/')\n",
    "        \n",
    "    ignore = []\n",
    "    c_l = []\n",
    "\n",
    "    for i, chunk in enumerate(audio_chunks):\n",
    "        nc+=1\n",
    "        # ignoring audio segments less than 2s in length b/c they don't contain enough info for emotion feature extraction\n",
    "        if len(chunk) < 2000:\n",
    "            ignore.append(i)\n",
    "            continue\n",
    "       # if len(chunk) > 60*1000 #if >1min, gcloud speech to text doesn't work\n",
    "            #1 split should typically do it...\n",
    "        #else:\n",
    "        #conssistency prosody/emotion etc etc\n",
    "        #no clear way hmm, stop coding, design halt freeze etc etc\n",
    "        out_file = data_folder+\"chunks/chunk{0}.wav\".format(i)\n",
    "        chunk.export(out_file, format=\"wav\")\n",
    "\n",
    "        ###### prosody features #######\n",
    "\n",
    "        prosody_features_filename = get_prosody_features_from_audio(out_file)\n",
    "        with open(prosody_features_filename, 'r') as f:\n",
    "            prosody_features = list(map(float, f.read()[:-1].split(' ')))\n",
    "        all_prosody_vectors.append(prosody_features)\n",
    "\n",
    "        ###### emotion features #######\n",
    "\n",
    "        converted_text = get_text_from_speech(out_file)\n",
    "        emoji_ids, emotion_features = get_emotion_features_from_text(converted_text, out_file)\n",
    "        all_emoji_ids.append(emoji_ids)\n",
    "        all_emotion_vectors.append(emotion_features)\n",
    "    \n",
    "    n_chunks.append(nc)\n",
    "        \n",
    "    audio_portions = detect_nonsilent(sound_file, min_silence_len=1000, silence_thresh=-50, seek_step=1)\n",
    "\n",
    "    skeleton_frames = []\n",
    "    skeleton_frames_idx = []\n",
    "    lc=[]\n",
    "    aud=[]\n",
    "    audio_portions_clf = {}  # 1=has info, 0=silent or ignored b/c <2s\n",
    "    for idx,pair in enumerate(audio_portions):\n",
    "        audio_portions_clf[tuple(pair)] = 1\n",
    "        if idx in ignore:\n",
    "            audio_portions_clf[tuple(pair)] = 0\n",
    "            continue\n",
    "        aud.append(tuple(pair))\n",
    "        start = pair[0]\n",
    "        end = pair[1]\n",
    "        fr = []\n",
    "        fr_idx = []\n",
    "        for idx,d in enumerate(skeleton_dict):\n",
    "            if d['time_ms'] >= start and d['time_ms'] <= end:\n",
    "                d_tmp = d.copy()\n",
    "                del d_tmp['time_ms']\n",
    "                fr.append(d_tmp)\n",
    "                fr_idx.append(idx)\n",
    "        lc.append(len(fr))\n",
    "        skeleton_frames.append(fr)\n",
    "        skeleton_frames_idx.append(fr_idx)\n",
    "    l_chunks.append(lc)\n",
    "    auds.append(aud)\n",
    "\n",
    "    with open(data_folder+'relative_joint_positions.json', 'w') as f:\n",
    "        json.dump(skeleton_frames, f)\n",
    "        #all on stilts...\n",
    "    silent_audio_portions = detect_silence(sound_file, min_silence_len=1000, silence_thresh=-50, seek_step=1)\n",
    "    for pair in silent_audio_portions:\n",
    "        audio_portions_clf[tuple(pair)] = 0\n",
    "    audio_portions_dicts.append(audio_portions_clf)\n",
    "\n",
    "    # convert feature vectors to np.ndarrays\n",
    "    prosody_vectors = np.array(all_prosody_vectors)\n",
    "\n",
    "    emotion_vectors = []\n",
    "    for vec in all_emotion_vectors:\n",
    "        mat = np.zeros((1, 64))\n",
    "        if len(vec) != 0:\n",
    "            for idx in range(len(vec)):\n",
    "                mat += vec[idx]\n",
    "        emotion_vectors.append(mat)\n",
    "    emotion_vectors = np.array(emotion_vectors)\n",
    "\n",
    "    combined_features = []\n",
    "    for idx,prosody_vec in enumerate(prosody_vectors):\n",
    "        emotion_vec = emotion_vectors[idx,:,:]\n",
    "        mat = np.zeros((emotion_vec.shape[0]+1, emotion_vec.shape[1]))\n",
    "        for i in range(mat.shape[0]-1):\n",
    "            mat[i,:]=emotion_vec[i,:]\n",
    "        mat[i+1,:]=np.pad(prosody_vec, (0,mat.shape[1]-len(prosody_vec)), 'constant', constant_values=(0))\n",
    "        combined_features.append(mat)\n",
    "    combined_features = np.array(combined_features)\n",
    "    \n",
    "    pros.append(prosody_vectors)\n",
    "    em.append(emotion_vectors)\n",
    "    combo.append(combined_features)\n",
    "        \n",
    "    skeleton_frame_to_audio_frame_idx_dict = {}\n",
    "    for idx, idx_list in enumerate(skeleton_frames_idx):\n",
    "        for i in idx_list:\n",
    "            if i not in skeleton_frame_to_audio_frame_idx_dict.keys():\n",
    "                skeleton_frame_to_audio_frame_idx_dict[i] = idx\n",
    "    \n",
    "    frames = []\n",
    "    joint_name_order = []\n",
    "    for frame in skeleton_dict:\n",
    "        fr = []\n",
    "        joint_name_order = []\n",
    "        for joint_name, joint_pos in frame.items():\n",
    "            joint_name_order.append(joint_name)\n",
    "            if joint_name == 'time_ms':\n",
    "                continue\n",
    "            fr.append(joint_pos)\n",
    "        frames.append(fr)\n",
    "    frames = np.array(frames)\n",
    "    \n",
    "    with open('joint_name_order.pkl', 'wb') as fp:\n",
    "        pickle.dump(joint_name_order, fp)\n",
    "    \n",
    "    # get distances from each dataset pose to each of the 10 base poses\n",
    "    dists = []\n",
    "    idx_mapper = {}\n",
    "    angles_final = get_joint_angles(final_poses)\n",
    "    angles_current = get_joint_angles(frames)\n",
    "    for pose_idx in range(angles_final.shape[0]):\n",
    "        pose = angles_final[pose_idx, :]\n",
    "        d = []\n",
    "        # find closest poses from data capture to this pose\n",
    "        for idx, skeleton_frame_idx in enumerate(skeleton_frame_to_audio_frame_idx_dict.keys()):\n",
    "            d.append(np.linalg.norm(pose - angles_current[skeleton_frame_idx, :]))  # what kind of dist?\n",
    "            idx_mapper[skeleton_frame_idx] = idx\n",
    "        dists.append(d)\n",
    "    dists = np.array(dists)\n",
    "    \n",
    "    # label each dataset pose with the index of the base pose it is closest to (euclidean distance between angles)\n",
    "    # convert label to one-hot\n",
    "    skeletal_frames_one_hot = []\n",
    "    pose_assignment = np.argmin(dists, axis=0)\n",
    "    for pose_idx in pose_assignment:\n",
    "        encoding = [0]*pose_idx + [1] + [0]*(dists.shape[0]-pose_idx-1)\n",
    "        skeletal_frames_one_hot.append(encoding)\n",
    "    skeletal_frames_one_hot = np.array(skeletal_frames_one_hot)\n",
    "    \n",
    "    # one feature vector can be associated with multiple and varying numbers of pose frames\n",
    "    #   because audio segments are >2s in length and joint positions vary within that time frame\n",
    "    # we want to label each feature vector with one pose vector, so replicate feature vectors for each pose\n",
    "    #   eg: if feature vector 1 is associated with 20 pose frames, replicate feature vector 1 20 times, once for each pose frame\n",
    "    input_emotion = []\n",
    "    input_prosody = []\n",
    "    input_combined = []\n",
    "    output_skeleton = []\n",
    "\n",
    "    for skeleton_frame_idx, audio_frame_idx in skeleton_frame_to_audio_frame_idx_dict.items():\n",
    "        emotion_v = emotion_vectors[audio_frame_idx]\n",
    "        prosody_v = prosody_vectors[audio_frame_idx]\n",
    "        combined_v = combined_features[audio_frame_idx]\n",
    "\n",
    "        skeleton_one_hot = skeletal_frames_one_hot[idx_mapper[skeleton_frame_idx]]\n",
    "\n",
    "        input_emotion.append(emotion_v.flatten())\n",
    "        input_prosody.append(prosody_v)\n",
    "        input_combined.append(combined_v.flatten())\n",
    "\n",
    "        output_skeleton.append(skeleton_one_hot)\n",
    "\n",
    "    input_emotion = np.array(input_emotion)\n",
    "    input_prosody = np.array(input_prosody)\n",
    "    input_combined = np.array(input_combined)\n",
    "\n",
    "    output_skeleton = np.array(output_skeleton)\n",
    "\n",
    "    print(input_emotion.shape, input_prosody.shape, input_combined.shape, output_skeleton.shape)\n",
    "    \n",
    "    X_emotion.append(input_emotion)\n",
    "    X_prosody.append(input_prosody)\n",
    "    X_combined.append(input_combined)\n",
    "\n",
    "    y_skeleton.append(output_skeleton)\n",
    "    \n",
    "X_emotion = np.array(X_emotion)\n",
    "X_prosody = np.array(X_prosody)\n",
    "X_combined = np.array(X_combined)\n",
    "\n",
    "y_skeleton = np.array(y_skeleton)\n",
    "\n",
    "# i don't remember why i did this, sooo i'll just leave it here so i don't break anything else\n",
    "# TODO: c'mon now\n",
    "# convert properly to np array?\n",
    "x_emo = X_emotion[0]\n",
    "x_pros = X_prosody[0]\n",
    "x_combo = X_combined[0]\n",
    "y_skel = y_skeleton[0]\n",
    "for idx in range(1, X_emotion.shape[0]):\n",
    "    x_emo = np.append(x_emo, X_emotion[idx], axis=0)\n",
    "    x_pros = np.append(x_pros, X_prosody[idx], axis=0)\n",
    "    x_combo = np.append(x_combo, X_combined[idx], axis=0)\n",
    "    y_skel = np.append(y_skel, y_skeleton[idx], axis=0)\n",
    "\n",
    "X_emotion = x_emo\n",
    "X_prosody = x_pros\n",
    "X_combined = x_combo\n",
    "X_combined = X_combined[:, :X_emotion.shape[1]+X_prosody.shape[1]]\n",
    "\n",
    "y_skeleton = y_skel\n",
    "\n",
    "print(X_emotion.shape, X_prosody.shape, X_combined.shape, y_skeleton.shape)\n",
    "\n",
    "#higher minsilencelength so fewer features...just have to split somehow hmm#adapt based on whenever doesnt work try smaller..for now hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total recording time = 26.188233333333336 minutes\n"
     ]
    }
   ],
   "source": [
    "print('Total recording time =', (sum(l_audio_chunks)/1000)/60, 'minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/Users/anuj/coursework_cuboulder/spring_2018/algo_hri/X_emotion.npy', X_emotion)\n",
    "np.save('/Users/anuj/coursework_cuboulder/spring_2018/algo_hri/X_prosody.npy', X_prosody)\n",
    "np.save('/Users/anuj/coursework_cuboulder/spring_2018/algo_hri/X_combined.npy', X_combined)\n",
    "np.save('/Users/anuj/coursework_cuboulder/spring_2018/algo_hri/y_skeleton.npy', y_skeleton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_emotion = np.load('/Users/anuj/coursework_cuboulder/spring_2018/algo_hri/X_emotion.npy')\n",
    "X_prosody = np.load('/Users/anuj/coursework_cuboulder/spring_2018/algo_hri/X_prosody.npy')\n",
    "X_combined = np.load('/Users/anuj/coursework_cuboulder/spring_2018/algo_hri/X_combined.npy')\n",
    "y_skeleton = np.load('/Users/anuj/coursework_cuboulder/spring_2018/algo_hri/y_skeleton.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JOINT ANGLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rightElbow',\n",
       " 'torso',\n",
       " 'leftFoot',\n",
       " 'neck',\n",
       " 'rightHand',\n",
       " 'leftHand',\n",
       " 'leftKnee',\n",
       " 'rightFoot',\n",
       " 'time_ms',\n",
       " 'leftElbow',\n",
       " 'leftHip',\n",
       " 'rightHip',\n",
       " 'leftShoulder',\n",
       " 'rightKnee',\n",
       " 'rightShoulder',\n",
       " 'head']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this allows us to preserve ordering between the pose dictionary keys and np.ndarray version of poses\n",
    "with open ('joint_name_order.pkl', 'rb') as fp:\n",
    "    joint_name_order = pickle.load(fp)\n",
    "joint_name_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_ms dictionary key causes indexing issues, so this is a workaround for that\n",
    "joint_name_idx_dict = {}\n",
    "flag = False\n",
    "for idx, joint_name in enumerate(joint_name_order):\n",
    "    if joint_name == 'time_ms':\n",
    "        flag = True\n",
    "        continue\n",
    "    if flag:\n",
    "        joint_name_idx_dict[joint_name] = idx-1\n",
    "    else:\n",
    "        joint_name_idx_dict[joint_name] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given 3 (x,y,z) coordinate tuples, calculate angle of vertex joint with correct sign\n",
    "# angle is defined in the plane formed by the 2 vectors originating from the vertex joint\n",
    "# cross product produces nan when all points in same line -- no need to worry because this is never the case with noisy measurements\n",
    "#   TODO: probably need to be fixed later for robustness\n",
    "def get_vertex_angle(joint1_pos, vertexjoint_pos, joint3_pos):\n",
    "    dist_1_vertex = joint1_pos - vertexjoint_pos\n",
    "    dist_2_vertex = joint3_pos - vertexjoint_pos\n",
    "    \n",
    "    x1, y1, z1 = dist_1_vertex\n",
    "    x2, y2, z2 = dist_2_vertex\n",
    "    \n",
    "    # getting the normal to the plane formed by the 2 vectors -- used for determining sign of angle\n",
    "    cross_prod = np.cross(dist_1_vertex, dist_2_vertex)\n",
    "    norm_vec = cross_prod / np.linalg.norm(cross_prod)\n",
    "    xn, yn, zn = norm_vec\n",
    "    \n",
    "    # https://stackoverflow.com/questions/14066933/direct-way-of-computing-clockwise-angle-between-2-vectors\n",
    "    dot = np.dot(dist_1_vertex, dist_2_vertex)\n",
    "    det = np.dot(norm_vec, cross_prod)\n",
    "    \n",
    "    angle = math.atan2(det, dot)\n",
    "    \n",
    "    return angle\n",
    "\n",
    "# dictionary inception\n",
    "def get_pos(joint_pos_dict, joint_name):\n",
    "    return joint_pos_dict[joint_name_idx_dict[joint_name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 9)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# angles are in radians, 9 total angles\n",
    "# TODO: should probably follow the right hand rule in angle naming convention for consistency's sake, but doesn't\n",
    "def get_joint_angles(poses_dict):\n",
    "    angles = []\n",
    "    angle_idx_dict = dict(zip(['neck', 'leftShoulder', 'rightShoulder', 'leftElbow', 'rightElbow', 'leftHip', 'rightHip', 'leftKnee', 'rightKnee'], range(0, 9)))\n",
    "\n",
    "    for i in range(poses_dict.shape[0]):\n",
    "        head_pos = get_pos(poses_dict[i], 'head')\n",
    "        neck_pos = get_pos(poses_dict[i], 'neck')\n",
    "        leftShoulder_pos = get_pos(poses_dict[i], 'leftShoulder')\n",
    "        rightShoulder_pos = get_pos(poses_dict[i], 'rightShoulder')\n",
    "        torso_pos = get_pos(poses_dict[i], 'torso')\n",
    "        leftElbow_pos = get_pos(poses_dict[i], 'leftElbow')\n",
    "        rightElbow_pos = get_pos(poses_dict[i], 'rightElbow')\n",
    "        leftHand_pos = get_pos(poses_dict[i], 'leftHand')\n",
    "        rightHand_pos = get_pos(poses_dict[i], 'rightHand')\n",
    "        leftHip_pos = get_pos(poses_dict[i], 'leftHip')\n",
    "        rightHip_pos = get_pos(poses_dict[i], 'rightHip')\n",
    "        leftKnee_pos = get_pos(poses_dict[i], 'leftKnee')\n",
    "        rightKnee_pos = get_pos(poses_dict[i], 'rightKnee')\n",
    "        leftFoot_pos = get_pos(poses_dict[i], 'leftFoot')\n",
    "        rightFoot_pos = get_pos(poses_dict[i], 'rightFoot')\n",
    "\n",
    "        head_neck_torso_angle = get_vertex_angle(head_pos, neck_pos, torso_pos)\n",
    "        neck_leftShoulder_leftElbow_angle = get_vertex_angle(neck_pos, leftShoulder_pos, leftElbow_pos)\n",
    "        neck_rightShoulder_rightElbow_angle = get_vertex_angle(neck_pos, rightShoulder_pos, rightElbow_pos)\n",
    "        leftShoulder_leftElbow_leftHand_angle = get_vertex_angle(leftShoulder_pos, leftElbow_pos, leftHand_pos)\n",
    "        rightShoulder_rightElbow_rightHand_angle = get_vertex_angle(rightShoulder_pos, rightElbow_pos, rightHand_pos)\n",
    "        torso_leftHip_leftKnee_angle = get_vertex_angle(torso_pos, leftHip_pos, leftKnee_pos)\n",
    "        torso_rightHip_rightKnee_angle = get_vertex_angle(torso_pos, rightHip_pos, rightKnee_pos)\n",
    "        leftHip_leftKnee_leftFoot_angle = get_vertex_angle(leftHip_pos, leftKnee_pos, leftFoot_pos)\n",
    "        rightHip_rightKnee_rightFoot_angle = get_vertex_angle(rightHip_pos, rightKnee_pos, rightFoot_pos)\n",
    "\n",
    "        angles.append([head_neck_torso_angle, neck_leftShoulder_leftElbow_angle, neck_rightShoulder_rightElbow_angle, leftShoulder_leftElbow_leftHand_angle, rightShoulder_rightElbow_rightHand_angle, torso_leftHip_leftKnee_angle, torso_rightHip_rightKnee_angle, leftHip_leftKnee_leftFoot_angle, rightHip_rightKnee_rightFoot_angle])\n",
    "\n",
    "    return np.array(angles)\n",
    "    \n",
    "angles = get_joint_angles(final_poses)\n",
    "\n",
    "#scaler = preprocessing.MinMaxScaler().fit(angles)\n",
    "#angles = scaler.transform(angles)\n",
    "\n",
    "angles.shape  # 10 base poses, 9 angles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REGRESSION APPROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_src = 'data5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.359936153232 0.270550678372 0.275339185954\n"
     ]
    }
   ],
   "source": [
    "def get_lr_score_and_predictions(features, test_arr):\n",
    "    X_train = np.nan_to_num(features[:-sum(l_chunks[-1])])\n",
    "    scaler = preprocessing.MinMaxScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "\n",
    "    y_train = np.where(y_skeleton[:-sum(l_chunks[-1])]==1)[1]\n",
    "\n",
    "    X_test = np.nan_to_num(features[-sum(l_chunks[-1]):])\n",
    "    scaler = preprocessing.MinMaxScaler().fit(X_test)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    y_test = np.where(y_skeleton[-sum(l_chunks[-1]):]==1)[1]\n",
    "\n",
    "    lr_clf = linear_model.LogisticRegression()\n",
    "    lr_clf.fit(X_train, y_train)\n",
    "    \n",
    "    test_arr = np.nan_to_num(test_arr)\n",
    "    scaler = preprocessing.MinMaxScaler().fit(test_arr)\n",
    "    test_arr = scaler.transform(test_arr)\n",
    "    \n",
    "    poses = lr_clf.predict(test_arr)\n",
    "\n",
    "    pp = []\n",
    "    aa=list(range(0,len(sound_file),5000))\n",
    "    aa.append(len(sound_file))\n",
    "    for idx in range(len(aa)-1):\n",
    "        duration = round((aa[idx+1] - aa[idx])/1000)\n",
    "        for i in range(duration):\n",
    "            if idx<len(poses):\n",
    "                pp.append(poses[idx])\n",
    "\n",
    "    return lr_clf.score(X_test, y_test), pp\n",
    "\n",
    "lr_em_score, lr_em_poses = get_lr_score_and_predictions(X_emotion, e)\n",
    "lr_pros_score, lr_pros_poses = get_lr_score_and_predictions(X_prosody, p)\n",
    "lr_combo_score, lr_combo_poses = get_lr_score_and_predictions(X_combined, c)\n",
    "\n",
    "print(lr_em_score, lr_pros_score, lr_combo_score)\n",
    "\n",
    "with open('poses_emotion_lr_'+audio_src+'.txt', 'w') as f:\n",
    "    for pose in lr_em_poses:\n",
    "        f.write(\"%s\\n\" % pose)\n",
    "\n",
    "with open('poses_prosody_lr_'+audio_src+'.txt', 'w') as f:\n",
    "    for pose in lr_pros_poses:\n",
    "        f.write(\"%s\\n\" % pose)\n",
    "        \n",
    "with open('poses_combined_lr_'+audio_src+'.txt', 'w') as f:\n",
    "    for pose in lr_combo_poses:\n",
    "        f.write(\"%s\\n\" % pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.421388667199 0.328810853951 0.216280925778\n"
     ]
    }
   ],
   "source": [
    "def get_knn_score_and_predictions(features, test_arr):\n",
    "    X_train = np.nan_to_num(features[:-sum(l_chunks[-1])])\n",
    "    scaler = preprocessing.MinMaxScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "\n",
    "    y_train = np.where(y_skeleton[:-sum(l_chunks[-1])]==1)[1]\n",
    "\n",
    "    X_test = np.nan_to_num(features[-sum(l_chunks[-1]):])\n",
    "    scaler = preprocessing.MinMaxScaler().fit(X_test)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    y_test = np.where(y_skeleton[-sum(l_chunks[-1]):]==1)[1]\n",
    "\n",
    "    knn_clf = neighbors.KNeighborsClassifier()\n",
    "    knn_clf.fit(X_train, y_train)\n",
    "    \n",
    "    test_arr = np.nan_to_num(test_arr)\n",
    "    scaler = preprocessing.MinMaxScaler().fit(test_arr)\n",
    "    test_arr = scaler.transform(test_arr)\n",
    "    \n",
    "    poses = knn_clf.predict(test_arr)\n",
    "\n",
    "    pp = []\n",
    "    aa=list(range(0,len(sound_file),5000))\n",
    "    aa.append(len(sound_file))\n",
    "    for idx in range(len(aa)-1):\n",
    "        duration = round((aa[idx+1] - aa[idx])/1000)\n",
    "        for i in range(duration):\n",
    "            if idx<len(poses):\n",
    "                pp.append(poses[idx])\n",
    "\n",
    "    return knn_clf.score(X_test, y_test), pp\n",
    "\n",
    "knn_em_score, knn_em_poses = get_knn_score_and_predictions(X_emotion, e)\n",
    "knn_pros_score, knn_pros_poses = get_knn_score_and_predictions(X_prosody, p)\n",
    "knn_combo_score, knn_combo_poses = get_knn_score_and_predictions(X_combined, c)\n",
    "\n",
    "print(knn_em_score, knn_pros_score, knn_combo_score)\n",
    "\n",
    "with open('poses_emotion_knn_'+audio_src+'.txt', 'w') as f:\n",
    "    for pose in knn_em_poses:\n",
    "        f.write(\"%s\\n\" % pose)\n",
    "\n",
    "with open('poses_prosody_knn_'+audio_src+'.txt', 'w') as f:\n",
    "    for pose in knn_pros_poses:\n",
    "        f.write(\"%s\\n\" % pose)\n",
    "        \n",
    "with open('poses_combined_knn_'+audio_src+'.txt', 'w') as f:\n",
    "    for pose in knn_combo_poses:\n",
    "        f.write(\"%s\\n\" % pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nbeFoK7wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13cb451d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = [lr_em_score, lr_pros_score, lr_combo_score]\n",
    "knn = [knn_em_score, knn_pros_score, knn_combo_score]\n",
    "lstm = [em_lstm_avg_acc, pros_lstm_avg_acc, combo_lstm_avg_acc]\n",
    "\n",
    "width = 0.15\n",
    "offset = 0.05\n",
    "plt.bar(np.array(range(3))-1.5*width+offset, knn, width, label='knn (k=5)', color=['lightblue']*3, edgecolor=['black']*3)\n",
    "#plt.bar(np.array(range(3))-width+offset, knn_noshuf, width, label='knn, not shuffled (k=5)', hatch='//', color=['lightblue']*3, edgecolor=['black']*3)\n",
    "plt.bar(np.array(range(3))+1.5*width-offset, lr, width, label='logistic regression', color=['orange']*3, edgecolor=['black']*3)\n",
    "plt.bar(np.array(range(3)), lstm, width, label='lstm', color=['lightgreen']*3, edgecolor=['black']*3)\n",
    "\n",
    "#plt.bar(np.array(range(3))+2*width-offset, logistic_noshuf, width, label='logistic regression, not shuffled', hatch='//', color=['orange']*3, edgecolor=['black']*3)\n",
    "plt.legend(bbox_to_anchor=(1.0, 1.0))\n",
    "plt.xticks(range(3), ['emotion (64)', 'prosody (13)', 'combined (77)'])\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('audio feature type (num features)')\n",
    "plt.title('8257 data points, feature values scaled to range [0,1]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seen before! etc test;test and train share prosody features etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " array([3864,  146, 1309,  916,  623,  779,   97,  335, 1701,  337]))"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.where(y_skeleton==1)[1], return_counts=True) #pose distrib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM APPROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct sign?\n",
    "def angle_error(y_true, y_pred):\n",
    "    return K.sqrt(K.sum(K.square(y_true-y_pred)))\n",
    "\n",
    "def get_angles_from_onehot(arr):\n",
    "    out = []\n",
    "    for i in range(arr.shape[0]):\n",
    "        notonehot = np.where(arr[i,:]==1)[0][0]\n",
    "        out.append(angles[notonehot,:])\n",
    "    return np.array(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10794, 1, 64) (10794, 1, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anuj/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:77: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/Users/anuj/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:77: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(return_sequences=False, input_shape=(None, 64), units=9, dropout=0.2, recurrent_dropout=0.2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_11 (LSTM)               (None, 9)                 2664      \n",
      "=================================================================\n",
      "Total params: 2,664\n",
      "Trainable params: 2,664\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      " - 3s - loss: 36.9786 - acc: 0.0232\n",
      "Epoch 2/10\n",
      " - 2s - loss: 30.5229 - acc: 0.1577\n",
      "Epoch 3/10\n",
      " - 2s - loss: 29.7659 - acc: 0.2150\n",
      "Epoch 4/10\n",
      " - 2s - loss: 29.5891 - acc: 0.2188\n",
      "Epoch 5/10\n",
      " - 2s - loss: 29.5007 - acc: 0.1946\n",
      "Epoch 6/10\n",
      " - 2s - loss: 29.4420 - acc: 0.1957\n",
      "Epoch 7/10\n",
      " - 2s - loss: 29.4013 - acc: 0.1911\n",
      "Epoch 8/10\n",
      " - 2s - loss: 29.3755 - acc: 0.1847\n",
      "Epoch 9/10\n",
      " - 2s - loss: 29.3557 - acc: 0.1787\n",
      "Epoch 10/10\n",
      " - 2s - loss: 29.3349 - acc: 0.1765\n",
      "0.156424581006\n",
      "0.1636073423782921\n",
      "0.14739670187141005\n",
      "(10794, 1, 13) (10794, 1, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anuj/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:77: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(return_sequences=False, input_shape=(None, 13), units=9, dropout=0.2, recurrent_dropout=0.2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_12 (LSTM)               (None, 9)                 828       \n",
      "=================================================================\n",
      "Total params: 828\n",
      "Trainable params: 828\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      " - 3s - loss: 35.9088 - acc: 0.0208\n",
      "Epoch 2/10\n",
      " - 2s - loss: 29.9749 - acc: 0.1521\n",
      "Epoch 3/10\n",
      " - 2s - loss: 29.4471 - acc: 0.1548\n",
      "Epoch 4/10\n",
      " - 2s - loss: 29.3261 - acc: 0.1513\n",
      "Epoch 5/10\n",
      " - 2s - loss: 29.2746 - acc: 0.1438\n",
      "Epoch 6/10\n",
      " - 2s - loss: 29.2442 - acc: 0.1471\n",
      "Epoch 7/10\n",
      " - 2s - loss: 29.2271 - acc: 0.1322\n",
      "Epoch 8/10\n",
      " - 2s - loss: 29.2160 - acc: 0.1446\n",
      "Epoch 9/10\n",
      " - 2s - loss: 29.2080 - acc: 0.1467\n",
      "Epoch 10/10\n",
      " - 2s - loss: 29.2033 - acc: 0.1412\n",
      "0.0\n",
      "0.1636073423782921\n",
      "0.14739670187141005\n",
      "(10794, 1, 77) (10794, 1, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anuj/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:77: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(return_sequences=False, input_shape=(None, 77), units=9, dropout=0.2, recurrent_dropout=0.2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_13 (LSTM)               (None, 9)                 3132      \n",
      "=================================================================\n",
      "Total params: 3,132\n",
      "Trainable params: 3,132\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      " - 3s - loss: 33.7044 - acc: 0.2128\n",
      "Epoch 2/10\n",
      " - 2s - loss: 29.4543 - acc: 0.1795\n",
      "Epoch 3/10\n",
      " - 2s - loss: 29.3057 - acc: 0.1599\n",
      "Epoch 4/10\n",
      " - 2s - loss: 29.2579 - acc: 0.1543\n",
      "Epoch 5/10\n",
      " - 2s - loss: 29.2310 - acc: 0.1473\n",
      "Epoch 6/10\n",
      " - 2s - loss: 29.2146 - acc: 0.1475\n",
      "Epoch 7/10\n",
      " - 2s - loss: 29.2070 - acc: 0.1431\n",
      "Epoch 8/10\n",
      " - 2s - loss: 29.2017 - acc: 0.1423\n",
      "Epoch 9/10\n",
      " - 2s - loss: 29.1951 - acc: 0.1401\n",
      "Epoch 10/10\n",
      " - 2s - loss: 29.1926 - acc: 0.1465\n",
      "0.270550678372\n",
      "0.1636073423782921\n",
      "0.14739670187141005\n"
     ]
    }
   ],
   "source": [
    "#clean up\n",
    "models = []\n",
    "loss_vals = []\n",
    "test_accs = []\n",
    "for i in [X_emotion, np.nan_to_num(X_prosody), np.nan_to_num(X_combined)]:\n",
    "    input1 = i[:-sum(l_chunks[-1])]\n",
    "    output1 = y_skeleton[:-sum(l_chunks[-1])]\n",
    "    scaler = preprocessing.MinMaxScaler().fit(input1)\n",
    "    input1 = scaler.transform(input1)\n",
    "\n",
    "    te_input = i[-sum(l_chunks[-1]):]\n",
    "    te_output = y_skeleton[-sum(l_chunks[-1]):]\n",
    "    scaler = preprocessing.MinMaxScaler().fit(te_input)\n",
    "    te_input = scaler.transform(te_input)\n",
    "\n",
    "    #print()\n",
    "    #X = i[:-sum(l_chunks[-1])]\n",
    "    #y = y_skeleton[:-sum(l_chunks[-1])] # normalize!\n",
    "    #print('asdfasdf',X.shape)\n",
    "\n",
    "    #scaler = preprocessing.MinMaxScaler().fit(X)\n",
    "    #X = scaler.transform(X)\n",
    "    X = input1\n",
    "    y = output1\n",
    "    \n",
    "    #scaler = preprocessing.MinMaxScaler().fit(y)\n",
    "    #y = scaler.transform(y)\n",
    "\n",
    "    #losing sequence if shuffle? only chunk at sequnce ends?\n",
    "    \n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=True, random_state = 0)\n",
    "    #te = []\n",
    "    #tr = []\n",
    "    #cv.\n",
    "    #for train_index, test_index in cv.split(X):\n",
    "    #    tr = train_index\n",
    "    #    te = test_index\n",
    "\n",
    "    #X_train = X[tr]\n",
    "    #y_train = y[tr]\n",
    "    #X_test = X[te]\n",
    "    #y_test = y[te]\n",
    "    #print(X_train.shape,X.shape)\n",
    "    X_train = X#[np.newaxis,:,:]\n",
    "    y_train = y#[np.newaxis,:]\n",
    "    X_test = te_input\n",
    "    y_test = te_output\n",
    "    \n",
    "    n = X_train.shape[0]\n",
    "    L = 1\n",
    "    X_train_seq = []\n",
    "    y_train_seq = []\n",
    "    X_test_seq=[]\n",
    "    y_test_seq=[]\n",
    "    for k in range(n - L + 1):\n",
    "        X_train_seq.append(X_train[k : k + L])\n",
    "        y_train_seq.append(y_train[k : k + L])\n",
    "\n",
    "    n = X_test.shape[0]\n",
    "    for k in range(n-L+1):\n",
    "        X_test_seq.append(X_test[k:k+L])\n",
    "        y_test_seq.append(y_test[k:k+L])\n",
    "\n",
    "    X_train_seq = np.array(X_train_seq)\n",
    "    y_train_seq = np.array(y_train_seq)\n",
    "    print(X_train_seq.shape,y_train_seq.shape)\n",
    "    X_test_seq = np.array(X_test_seq)\n",
    "    y_test_seq = np.array(y_test_seq)\n",
    "    \n",
    "    lstm_out = 196\n",
    "    batch_size = 32\n",
    "    \n",
    "    yy = get_angles_from_onehot(y_train_seq)\n",
    "    yy1 = get_angles_from_onehot(y_test_seq)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(input_dim = X_train.shape[1], output_dim = yy.shape[1], dropout_U = 0.2, dropout_W = 0.2, return_sequences=False))\n",
    "    #model.add(LSTM(100, return_sequences=False))\n",
    "    #model.add(Dense(output_dim=yy.shape[1],activation='softmax'))\n",
    "    model.compile(loss =angle_error, optimizer='adam',metrics = ['accuracy']) #try mse?\n",
    "    #what else\n",
    "    print(model.summary())\n",
    "    \n",
    "    \n",
    "    \n",
    "    a=model.fit(X_train_seq, yy, batch_size = batch_size, epochs = 10, verbose = 2)\n",
    "    loss_vals.append(a)\n",
    "    score,acc = model.evaluate(X_test_seq, yy1, verbose = 2, batch_size = batch_size)\n",
    "    test_accs.append(acc)\n",
    "    print(acc)\n",
    "    acc=0\n",
    "    for ii in range(te_input.shape[0]):\n",
    "        output_frame_angles = model.predict(te_input[ii:ii+1,np.newaxis,:])\n",
    "        dists_output_to_base_poses = np.sqrt(np.sum(np.square(output_frame_angles - angles),axis=1))\n",
    "        output_pose_id = np.argmin(dists_output_to_base_poses)\n",
    "        #print(output_pose_id,np.where(te_output[ii:ii+1,:]==1)[1][0])\n",
    "        if output_pose_id == np.where(te_output[ii:ii+1,:]==1)[1][0]:\n",
    "            acc+=1\n",
    "    acc/=te_input.shape[0]*1.0\n",
    "    #score,acc = model.evaluate(X_test_seq, yy1, verbose = 2, batch_size = batch_size)\n",
    "    #print(score,acc)\n",
    "    print(acc)\n",
    "    acc=0\n",
    "    for ii in range(input1.shape[0]):\n",
    "        output_frame_angles = model.predict(input1[ii:ii+1,np.newaxis,:])\n",
    "        dists_output_to_base_poses = np.sqrt(np.sum(np.square(output_frame_angles - angles),axis=1))\n",
    "        output_pose_id = np.argmin(dists_output_to_base_poses)\n",
    "        #print(output_pose_id,np.where(te_output[ii:ii+1,:]==1)[1][0])\n",
    "        if output_pose_id == np.where(output1[ii:ii+1,:]==1)[1][0]:\n",
    "            acc+=1\n",
    "    acc/=input1.shape[0]*1.0\n",
    "    print(acc)\n",
    "    \n",
    "    #print(X_test_seq[0:1,:,:].shape)\n",
    "    #output_frame_angles = model.predict(X_test_seq[0:1,:,:])\n",
    "    #dists_output_to_base_poses = np.sqrt(np.sum(np.square(output_frame_angles - angles),axis=1))\n",
    "    #output_pose_id = np.argmin(dists_output_to_base_poses)\n",
    "    #print(output_pose_id)\n",
    "    #feed in by dict hmm\n",
    "    \n",
    "    models.append(model)\n",
    "    #momentum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.17359644247286954, 0.13346303502304457, 0.15731888086194573)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em_lstm_avg_acc = np.mean(loss_vals[0].history['acc'])\n",
    "pros_lstm_avg_acc = np.mean(loss_vals[1].history['acc'])\n",
    "combo_lstm_avg_acc = np.mean(loss_vals[2].history['acc'])\n",
    "\n",
    "em_lstm_avg_acc, pros_lstm_avg_acc, combo_lstm_avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAccAAAEWCAYAAAATnlw4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x137116898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "em_lstm_loss = loss_vals[0].history['loss']\n",
    "pros_lstm_loss = loss_vals[1].history['loss']\n",
    "combo_lstm_loss = loss_vals[2].history['loss']\n",
    "\n",
    "plt.plot(em_lstm_loss, label='emotion features')\n",
    "plt.plot(pros_lstm_loss, label='prosody features')\n",
    "plt.plot(combo_lstm_loss, label='combined features')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('angle loss')\n",
    "plt.title('training loss for LSTM by input feature type (10 epochs, ADAM optimizer)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.294726039560629"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em_lstm_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3+4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HUMAN PLAYBACK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a proportional number of sequential frames from each of the 14 audio chunks etc skel... random spaced etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dist based...angular? or euc?\n",
    "#CHANGE ALL TO ANGULAR?!\n",
    "#always rest gesture at end?\n",
    "#start at base pose hmm\n",
    "\n",
    "#use time_ms? too small hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_src='data5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "poses = np.where(y_skel[-sum(l_chunks[-1]):]==1)[1]\n",
    "\n",
    "pp = []\n",
    "prev_idx = 0\n",
    "idx1=-1\n",
    "dd=[]\n",
    "for idx, audio_start_end_pair in enumerate(audio_portions_dicts[-1].keys()):\n",
    "    flag = audio_portions_dicts[-1][audio_start_end_pair]\n",
    "    duration = round((audio_start_end_pair[1] - audio_start_end_pair[0])/1000)\n",
    "    #rounding adds extra frames...\n",
    "    dd.append(duration)\n",
    "    if flag == 1:\n",
    "        idx1+=1\n",
    "        \n",
    "        n = l_chunks[-1][idx1]\n",
    "        list_of_poses = poses[prev_idx:prev_idx+n]\n",
    "        sampled_pose_idxs_relative = sorted(np.random.choice(len(list_of_poses), duration, replace=False))\n",
    "        sampled_pose_idxs_absolute = [p+prev_idx for p in sampled_pose_idxs_relative]\n",
    "        for v in poses[sampled_pose_idxs_absolute]:\n",
    "            pp.append(v)\n",
    "        prev_idx += n\n",
    "    else:\n",
    "        for x in range(duration):\n",
    "            pp.append(2)\n",
    "\n",
    "with open('poses_humanreplay_'+audio_src+'.txt', 'w') as f:\n",
    "    for pose in pp:\n",
    "        f.write(\"%s\\n\" % pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pp)\n",
    "#all same at end because not enough long chunks...or silence etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#histograph of poses used hmm\n",
    "#too many same poses hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce according to audio length...\n",
    "#store all intermed vals etc\n",
    "#smallest time interval for nao?, smaller more better movements etc, too fast hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do it that proportion of time not exactly number of times--feed in nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[175589, 95434, 147725, 113267, 360281, 678998]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_audio_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM PLAYBACK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do based on pose frequency?\n",
    "\n",
    "poses = np.random.randint(0, 10, round(l_audio_chunks[-1]/1000))\n",
    "\n",
    "with open('poses_random_'+audio_src+'.txt', 'w') as f:\n",
    "    for pose in poses:\n",
    "        f.write(\"%s\\n\" % pose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NO GESTURES PLAYBACK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "poses = [2] * round(l_audio_chunks[-1]/1000)\n",
    "\n",
    "with open('poses_none_'+audio_src+'.txt', 'w') as f:\n",
    "    for pose in poses:\n",
    "        f.write(\"%s\\n\" % pose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROSODY PLAYBACK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-254-694b20d1a6e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mclosest_orig_vec_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdists_new_audio_feature_to_existing_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m#get timing info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mnum_frames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msame_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclosest_orig_vec_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#get indices of same feature vectors\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "existing_vectors = X_prosody[:-sum(l_chunks[-1])]\n",
    "dists1 = cdist(existing_vectors, existing_vectors, 'euclidean')\n",
    "\n",
    "same_indices = []\n",
    "for i in range(dists1.shape[0]):\n",
    "    same_indices.append(np.where(dists1[i]==0)[0])\n",
    "\n",
    "# find closest audio feature vector from existing dataset to new audio to get freq of gen!,,,,same in same out etc\n",
    "\n",
    "#feed num_frames times into NN\n",
    "num_frames = []\n",
    "\n",
    "feature_vectors = np.nan_to_num(prosody_vectors)\n",
    "existing_vectors = np.nan_to_num(X_prosody)\n",
    "\n",
    "for i in range(feature_vectors.shape[0]):\n",
    "    dists_new_audio_feature_to_existing_dataset = np.sqrt(np.sum(np.square(feature_vectors[i,:] - existing_vectors),axis=1))\n",
    "    closest_orig_vec_idx = np.argmin(dists_new_audio_feature_to_existing_dataset)\n",
    "    #get timing info\n",
    "    num_frames.append(len(same_indices[closest_orig_vec_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10107, 13)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_prosody.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assuming one second per pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n"
     ]
    }
   ],
   "source": [
    "poses = []\n",
    "\n",
    "#model = models[1]\n",
    "\n",
    "features = np.nan_to_num(p)\n",
    "scaler = preprocessing.MinMaxScaler().fit(features)\n",
    "features = scaler.transform(features)\n",
    "\n",
    "for i in range(p.shape[0]):\n",
    "    output_frame_angles = model.predict(features[np.newaxis,i:i+1,:])\n",
    "    dists_output_to_base_poses = np.sqrt(np.sum(np.square(output_frame_angles - angles),axis=1))\n",
    "    output_pose_id = np.argmin(dists_output_to_base_poses)\n",
    "    poses.append(output_pose_id)\n",
    "\n",
    "with open('poses_prosody_'+audio_src+'.txt', 'w') as f:\n",
    "    for pose in poses:\n",
    "        f.write(\"%s\\n\" % pose)\n",
    "        \n",
    "print(len(poses),poses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([7, 0, 0, 7, 2, 0, 0, 7, 0, 0, 0, 3, 7, 0, 7, 7, 7, 0, 0, 8, 7, 7, 0,\n",
       "        7, 0, 7, 7, 0, 7, 0, 7, 0, 8, 7, 7, 0, 5, 7, 0, 0, 7, 7, 0, 7, 8, 0,\n",
       "        4, 0, 0, 7, 7, 7, 4, 2, 0, 0, 7, 0, 0, 8, 7, 0, 2, 2, 0, 0, 0, 7, 0,\n",
       "        0, 7, 7, 3, 7, 7, 0, 0, 2, 3, 0, 3, 8, 7, 0, 7, 0, 7, 2, 8, 3, 8, 0,\n",
       "        0, 0, 0, 7, 0, 3, 7, 7, 0, 0, 0, 7, 0, 8, 0, 0, 0, 2, 7, 7, 0, 0, 7,\n",
       "        0, 7, 7, 0, 2, 0, 7, 0, 7, 0, 0, 7, 0, 2, 7, 7, 7, 7, 7, 7]),\n",
       " 675,\n",
       " 678998)"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.predict(np.nan_to_num(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "636 678998\n"
     ]
    }
   ],
   "source": [
    "poses = []\n",
    "\n",
    "# take best approach lstm or otherwise and feed in hmm\n",
    "\n",
    "#angle compare also scale, see output of NN!\n",
    "\n",
    "\n",
    "a=neighbors.KNeighborsClassifier(n_neighbors=18, leaf_size=100)\n",
    "input1 = np.nan_to_num(X_prosody[:-sum(l_chunks[-1])])\n",
    "output1 = np.where(y_skeleton[:-sum(l_chunks[-1])]==1)[1]\n",
    "a.fit(input1, output1)\n",
    "#print(a.score(X_emotion[-sum(l_chunks[-1]):], np.where(y_skeleton[-sum(l_chunks[-1]):]==1)[1]))\n",
    "\n",
    "\n",
    "\n",
    "#model = models[1]\n",
    "\n",
    "features = np.nan_to_num(X_prosody[-sum(l_chunks[-1]):])\n",
    "scaler = preprocessing.MinMaxScaler().fit(features)\n",
    "features = scaler.transform(features)\n",
    "poses = []\n",
    "idx_new=0\n",
    "counter=-1\n",
    "#find closest audio vec in dataset instead of assuming you have in dataset, remove from og dataset what used etc, precaution etc\n",
    "# should ideally do by sample frames based on dataset stats but need timings to match so using duration\n",
    "#approach\n",
    "for idx,k in enumerate(audio_portions_dicts[-1].keys()):\n",
    "    v=audio_portions_dicts[-1][k]\n",
    "    if v==1:\n",
    "        counter+=1\n",
    "        \n",
    "        #print(output_frame_angles)\n",
    "        #rescale output?\n",
    "        #scaler = preprocessing.MinMaxScaler().fit(output_frame_angles)\n",
    "        #output_frame_angles = scaler.transform(output_frame_angles)\n",
    "        #print(output_frame_angles)\n",
    "        \n",
    "        for i in range(round((k[1]-k[0])/1000)):\n",
    "            output_frame_angles = a.predict(features[idx_new:idx_new+1,:])#model.predict(features[np.newaxis,idx_new:idx_new+1,:])\n",
    "            #dists_output_to_base_poses = np.sqrt(np.sum(np.square(output_frame_angles - angles),axis=1))\n",
    "            #output_pose_id = np.argmin(dists_output_to_base_poses)\n",
    "            poses.append(output_pose_id)\n",
    "        idx_new += l_chunks[-1][counter]\n",
    "    else:\n",
    "        poses.append(2)\n",
    "        \n",
    "        \n",
    "        #scale and squash dim!\n",
    "print(len(poses), l_audio_chunks[-1])\n",
    "\n",
    "with open('poses_prosody_'+audio_src+'.txt', 'w') as f:\n",
    "    for pose in poses:\n",
    "        f.write(\"%s\\n\" % pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2]"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 145.13421631   23.34489059   65.70452157 ...,    1.54056949\n",
      "   169.16666667  166.18889855]\n",
      " [ 145.13421631   23.34489059   65.70452157 ...,    1.54056949\n",
      "   169.16666667  166.18889855]\n",
      " [ 145.13421631   23.34489059   65.70452157 ...,    1.54056949\n",
      "   169.16666667  166.18889855]\n",
      " ..., \n",
      " [ 169.63633728   24.42424202   67.26948868 ...,    1.97960408\n",
      "   114.84848485  112.89902198]\n",
      " [ 169.63633728   24.42424202   67.26948868 ...,    1.97960408\n",
      "   114.84848485  112.89902198]\n",
      " [ 169.63633728   24.42424202   67.26948868 ...,    1.97960408\n",
      "   114.84848485  112.89902198]]\n",
      "[[ 184.2086792    22.18695831   62.94304324 ...,    1.90636911\n",
      "   128.26923077  164.32520704]\n",
      " [ 155.79295349   20.68192101   60.51084237 ...,    1.53735371  153.125\n",
      "   137.87307342]\n",
      " [ 186.13644409   33.0322113    78.72292297 ...,    1.41716135\n",
      "   195.45454545  228.44787097]\n",
      " ..., \n",
      " [ 152.14593506   19.78505325   58.97582048 ...,    1.23254362\n",
      "   276.66666667  325.35450751]\n",
      " [ 143.80644226   20.20790863   59.70803822 ...,    1.61632451  163.125\n",
      "   175.5248825 ]\n",
      " [ 155.79295349   20.68192101   60.51084237 ...,    1.53735371  153.125\n",
      "   137.87307342]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-508-d0d5da557b77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0moutput1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom_inds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_emotion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_chunks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_skeleton\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_chunks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/anuj/anaconda/lib/python3.6/site-packages/sklearn/neighbors/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \"\"\"\n\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKDTree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBallTree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/anuj/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    571\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[1;32m    572\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[1;32m    574\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m/Users/anuj/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    451\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/anuj/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     42\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     43\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 44\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "a=neighbors.KNeighborsClassifier(n_neighbors=18, leaf_size=100)\n",
    "input1 = X_prosody[:-sum(l_chunks[-1])]\n",
    "print(input1)\n",
    "random_inds = np.random.permutation(list(range(input1.shape[0])))\n",
    "output1 = np.where(y_skeleton[:-sum(l_chunks[-1])]==1)[1]\n",
    "input1 = input1[random_inds]\n",
    "print(input1)\n",
    "output1 = output1[random_inds]\n",
    "a.fit(input1, output1)\n",
    "print(a.score(X_emotion[-sum(l_chunks[-1]):], np.where(y_skeleton[-sum(l_chunks[-1]):]==1)[1]))\n",
    "\n",
    "#a.fit(X_emotion[:-sum(l_chunks[-1])], np.where(y_skeleton[:-sum(l_chunks[-1])]==1)[1])\n",
    "a.predict(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num frames or timing based approach hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure naoqi audio file corresponds etc\n",
    "#lots of silence, change nums hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2]), array([223,   2,  27]))"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_src='data6'\n",
    "np.unique(poses,return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMOTION PLAYBACK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 3, 8, 8, 3, 8, 8, 8, 8, 8, 8, 8, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 3, 8, 8, 3, 8, 8, 8]\n"
     ]
    }
   ],
   "source": [
    "poses = []\n",
    "\n",
    "model = models[0]\n",
    "\n",
    "features = np.nan_to_num(e)\n",
    "scaler = preprocessing.MinMaxScaler().fit(features)\n",
    "features = scaler.transform(features)\n",
    "\n",
    "for i in range(p.shape[0]):\n",
    "    output_frame_angles = model.predict(features[np.newaxis,i:i+1,:])\n",
    "    dists_output_to_base_poses = np.sqrt(np.sum(np.square(output_frame_angles - angles),axis=1))\n",
    "    output_pose_id = np.argmin(dists_output_to_base_poses)\n",
    "    poses.append(output_pose_id)\n",
    "\n",
    "with open('poses_emotion_'+audio_src+'.txt', 'w') as f:\n",
    "    for pose in poses:\n",
    "        f.write(\"%s\\n\" % pose)\n",
    "        \n",
    "print(len(poses),poses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252 261875\n"
     ]
    }
   ],
   "source": [
    "poses = []\n",
    "\n",
    "model = models[0]\n",
    "\n",
    "features = np.nan_to_num(X_emotion[-sum(l_chunks[-1]):])\n",
    "scaler = preprocessing.MinMaxScaler().fit(features)\n",
    "features = scaler.transform(features)\n",
    "poses = []\n",
    "idx_new=0\n",
    "counter=-1\n",
    "#find closest audio vec in dataset instead of assuming you have in dataset, remove from og dataset what used etc, precaution etc\n",
    "# should ideally do by sample frames based on dataset stats but need timings to match so using duration\n",
    "#approach\n",
    "for idx,k in enumerate(audio_portions_dicts[-1].keys()):\n",
    "    v=audio_portions_dicts[-1][k]\n",
    "    if v==1:\n",
    "        counter+=1\n",
    "        \n",
    "        #print(output_frame_angles)\n",
    "        #rescale output?\n",
    "        #scaler = preprocessing.MinMaxScaler().fit(output_frame_angles)\n",
    "        #output_frame_angles = scaler.transform(output_frame_angles)\n",
    "        #print(output_frame_angles)\n",
    "        \n",
    "        for i in range(round((k[1]-k[0])/1000)):\n",
    "            output_frame_angles = model.predict(features[np.newaxis,idx_new:idx_new+1,:])\n",
    "            dists_output_to_base_poses = np.sqrt(np.sum(np.square(output_frame_angles - angles),axis=1))\n",
    "            output_pose_id = np.argmin(dists_output_to_base_poses)\n",
    "            poses.append(output_pose_id)\n",
    "        idx_new += l_chunks[-1][counter]\n",
    "    else:\n",
    "        poses.append(2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #scale and squash dim!\n",
    "print(len(poses), l_audio_chunks[-1])\n",
    "\n",
    "with open('poses_emotion_'+audio_src+'.txt', 'w') as f:\n",
    "    for pose in poses:\n",
    "        f.write(\"%s\\n\" % pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2]), array([200,  25,  27]))"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(poses,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anuj/anaconda/lib/python3.6/site-packages/torch/nn/modules/container.py:67: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "8267\n"
     ]
    }
   ],
   "source": [
    "pro1 = []\n",
    "ids1=[]\n",
    "emo1=[]\n",
    "len_chunks=[]\n",
    "mono_audio_filename = '/Users/anuj/coursework_cuboulder/spring_2018/algo_hri/kinect_data/data5/mono_myrecording.wav'\n",
    "sound_file = AudioSegment.from_wav(mono_audio_filename)\n",
    "a=range(0,len(sound_file),5*1000)\n",
    "for idx,i in enumerate(a):\n",
    "    out_file = '/Users/anuj/coursework_cuboulder/spring_2018/algo_hri/kinect_data/data5/chunks/chunk_{0}.wav'.format(i)\n",
    "    if idx==len(a)-2:\n",
    "        chunk=sound_file[i:]\n",
    "    else:\n",
    "        chunk = sound_file[i:i+5*1000]\n",
    "    len_chunks.append(len(chunk))\n",
    "    print(len(chunk))\n",
    "    chunk.export(out_file, format=\"wav\")\n",
    "    prosody_features_filename = get_prosody_features_from_audio(out_file)\n",
    "    #print(prosody_features_filename)\n",
    "    with open(prosody_features_filename, 'r') as f:\n",
    "        prosody_features = list(map(float, f.read()[:-1].split(' ')))\n",
    "    pro1.append(prosody_features)\n",
    "\n",
    "    converted_text = get_text_from_speech(out_file)\n",
    "    emoji_ids, emotion_features = get_emotion_features_from_text(converted_text, out_file)\n",
    "    ids1.append(emoji_ids)\n",
    "    emo1.append(emotion_features)\n",
    "    \n",
    "    if idx==len(a)-2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro1=np.array(pro1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_vectors1 = []\n",
    "for vec in emo1:\n",
    "    mat = np.zeros((1, 64))\n",
    "    if len(vec) != 0:\n",
    "        for idx in range(len(vec)):\n",
    "            mat += vec[idx]\n",
    "    emotion_vectors1.append(mat)\n",
    "emotion_vectors1 = np.array(emotion_vectors1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_features1 = []\n",
    "for idx,prosody_vec in enumerate(pro1):\n",
    "    emotion_vec = emotion_vectors1[idx,:,:]\n",
    "    mat = np.zeros((emotion_vec.shape[0]+1, emotion_vec.shape[1]))\n",
    "    for i in range(mat.shape[0]-1):\n",
    "        mat[i,:]=emotion_vec[i,:]\n",
    "    mat[i+1,:]=np.pad(prosody_vec, (0,mat.shape[1]-len(prosody_vec)), 'constant', constant_values=(0))\n",
    "    combined_features1.append(mat)\n",
    "combined_features1 = np.array(combined_features1)\n",
    "#combined_features1 = combined_features1[:, :emotion_vectors1.shape[1]+pro1.shape[1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((22, 1, 64), (22, 13), (22, 2, 64))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_vectors1.shape,pro1.shape,combined_features1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what about last x chunk? combine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((22, 64), (22, 13), (22, 77))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=[]\n",
    "e=[]\n",
    "c=[]\n",
    "for i in range(pro1.shape[0]):\n",
    "    emotion_v = emotion_vectors1[i]\n",
    "    prosody_v = pro1[i]\n",
    "    combined_v = combined_features1[i]\n",
    "\n",
    "    e.append(emotion_v.flatten())\n",
    "    p.append(prosody_v)\n",
    "    c.append(combined_v.flatten())\n",
    "e = np.array(e)\n",
    "p = np.array(p)\n",
    "c = np.array(c)\n",
    "c=c[:, :e.shape[1]+p.shape[1]]\n",
    "e.shape,p.shape,c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252 261875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0, 2, 3, 5, 9]), array([164,  38,  15,  23,  12]))"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poses = []\n",
    "\n",
    "model = a\n",
    "\n",
    "features = np.nan_to_num(X_emotion[-sum(l_chunks[-1]):])\n",
    "scaler = preprocessing.MinMaxScaler().fit(features)\n",
    "features = scaler.transform(features)\n",
    "poses = []\n",
    "idx_new=0\n",
    "counter=-1\n",
    "#find closest audio vec in dataset instead of assuming you have in dataset, remove from og dataset what used etc, precaution etc\n",
    "# should ideally do by sample frames based on dataset stats but need timings to match so using duration\n",
    "#approach\n",
    "for idx,k in enumerate(audio_portions_dicts[-1].keys()):\n",
    "    v=audio_portions_dicts[-1][k]\n",
    "    if v==1:\n",
    "        counter+=1\n",
    "        \n",
    "        #print(output_frame_angles)\n",
    "        #rescale output?\n",
    "        #scaler = preprocessing.MinMaxScaler().fit(output_frame_angles)\n",
    "        #output_frame_angles = scaler.transform(output_frame_angles)\n",
    "        #print(output_frame_angles)\n",
    "        \n",
    "        #same prosody same output hmm\n",
    "        #if testing and chunking by prosody then doesnt work for emotion hmm\n",
    "        # for testing, split audio by seconds, more for emotion obviously, get more variation in pose...\n",
    "        #to make work for current timing idea etc\n",
    "        for i in range(round((k[1]-k[0])/1000)):\n",
    "            output_frame_angles = a.predict(features[idx_new:idx_new+1,:])#model.predict(features[np.newaxis,idx_new:idx_new+1,:]) #train every time hmm\n",
    "            #print(output_frame_angles)\n",
    "            #dists_output_to_base_poses = np.sqrt(np.sum(np.square(output_frame_angles - angles),axis=1))\n",
    "            #output_pose_id = np.argmin(dists_output_to_base_poses)\n",
    "            poses.append(output_frame_angles[0])\n",
    "        #print('\\n')\n",
    "        idx_new += l_chunks[-1][counter]\n",
    "    else:\n",
    "        poses.append(2)\n",
    "        \n",
    "        #SAME IN SAME OUT\n",
    "        \n",
    "        #scale and squash dim!\n",
    "print(len(poses), l_audio_chunks[-1])\n",
    "\n",
    "with open('poses_emotion_'+audio_src+'.txt', 'w') as f:\n",
    "    for pose in poses:\n",
    "        f.write(\"%s\\n\" % pose)\n",
    "        \n",
    "np.unique(poses,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2]"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROSODY+EMOTION PLAYBACK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n"
     ]
    }
   ],
   "source": [
    "poses = []\n",
    "\n",
    "model = models[2]\n",
    "\n",
    "features = np.nan_to_num(c)\n",
    "scaler = preprocessing.MinMaxScaler().fit(features)\n",
    "features = scaler.transform(features)\n",
    "\n",
    "for i in range(p.shape[0]):\n",
    "    output_frame_angles = model.predict(features[np.newaxis,i:i+1,:])\n",
    "    dists_output_to_base_poses = np.sqrt(np.sum(np.square(output_frame_angles - angles),axis=1))\n",
    "    output_pose_id = np.argmin(dists_output_to_base_poses)\n",
    "    poses.append(output_pose_id)\n",
    "\n",
    "with open('poses_combined_'+audio_src+'.txt', 'w') as f:\n",
    "    for pose in poses:\n",
    "        f.write(\"%s\\n\" % pose)\n",
    "        \n",
    "print(len(poses),poses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.75954324  0.26460454  0.30748951  0.22519715  0.26152191 -0.18687975\n",
      "   0.52609086  0.75803256  0.76036006]]\n",
      "[[ 0.75954324  0.26460454  0.30748951  0.22519715  0.26152191 -0.18687975\n",
      "   0.52609086  0.75803256  0.76036006]]\n",
      "[[ 0.75954324  0.26460454  0.30748951  0.22519715  0.26152191 -0.18687975\n",
      "   0.52609086  0.75803256  0.76036006]]\n",
      "[[ 0.75954324  0.26460454  0.30748951  0.22519715  0.26152191 -0.18687975\n",
      "   0.52609086  0.75803256  0.76036006]]\n",
      "\n",
      "\n",
      "[[ 0.75933421  0.26332188  0.29656428  0.22593507  0.26237652 -0.22707544\n",
      "   0.52137446  0.75707036  0.75996971]]\n",
      "[[ 0.75933421  0.26332188  0.29656428  0.22593507  0.26237652 -0.22707544\n",
      "   0.52137446  0.75707036  0.75996971]]\n",
      "[[ 0.75933421  0.26332188  0.29656428  0.22593507  0.26237652 -0.22707544\n",
      "   0.52137446  0.75707036  0.75996971]]\n",
      "[[ 0.75933421  0.26332188  0.29656428  0.22593507  0.26237652 -0.22707544\n",
      "   0.52137446  0.75707036  0.75996971]]\n",
      "[[ 0.75933421  0.26332188  0.29656428  0.22593507  0.26237652 -0.22707544\n",
      "   0.52137446  0.75707036  0.75996971]]\n",
      "[[ 0.75933421  0.26332188  0.29656428  0.22593507  0.26237652 -0.22707544\n",
      "   0.52137446  0.75707036  0.75996971]]\n",
      "[[ 0.75933421  0.26332188  0.29656428  0.22593507  0.26237652 -0.22707544\n",
      "   0.52137446  0.75707036  0.75996971]]\n",
      "[[ 0.75933421  0.26332188  0.29656428  0.22593507  0.26237652 -0.22707544\n",
      "   0.52137446  0.75707036  0.75996971]]\n",
      "[[ 0.75933421  0.26332188  0.29656428  0.22593507  0.26237652 -0.22707544\n",
      "   0.52137446  0.75707036  0.75996971]]\n",
      "[[ 0.75933421  0.26332188  0.29656428  0.22593507  0.26237652 -0.22707544\n",
      "   0.52137446  0.75707036  0.75996971]]\n",
      "[[ 0.75933421  0.26332188  0.29656428  0.22593507  0.26237652 -0.22707544\n",
      "   0.52137446  0.75707036  0.75996971]]\n",
      "[[ 0.75933421  0.26332188  0.29656428  0.22593507  0.26237652 -0.22707544\n",
      "   0.52137446  0.75707036  0.75996971]]\n",
      "\n",
      "\n",
      "[[ 0.75841635  0.26305759  0.29916877  0.22346468  0.2610915  -0.20575389\n",
      "   0.51835018  0.75718707  0.76032811]]\n",
      "[[ 0.75841635  0.26305759  0.29916877  0.22346468  0.2610915  -0.20575389\n",
      "   0.51835018  0.75718707  0.76032811]]\n",
      "[[ 0.75841635  0.26305759  0.29916877  0.22346468  0.2610915  -0.20575389\n",
      "   0.51835018  0.75718707  0.76032811]]\n",
      "[[ 0.75841635  0.26305759  0.29916877  0.22346468  0.2610915  -0.20575389\n",
      "   0.51835018  0.75718707  0.76032811]]\n",
      "[[ 0.75841635  0.26305759  0.29916877  0.22346468  0.2610915  -0.20575389\n",
      "   0.51835018  0.75718707  0.76032811]]\n",
      "[[ 0.75841635  0.26305759  0.29916877  0.22346468  0.2610915  -0.20575389\n",
      "   0.51835018  0.75718707  0.76032811]]\n",
      "[[ 0.75841635  0.26305759  0.29916877  0.22346468  0.2610915  -0.20575389\n",
      "   0.51835018  0.75718707  0.76032811]]\n",
      "[[ 0.75841635  0.26305759  0.29916877  0.22346468  0.2610915  -0.20575389\n",
      "   0.51835018  0.75718707  0.76032811]]\n",
      "[[ 0.75841635  0.26305759  0.29916877  0.22346468  0.2610915  -0.20575389\n",
      "   0.51835018  0.75718707  0.76032811]]\n",
      "[[ 0.75841635  0.26305759  0.29916877  0.22346468  0.2610915  -0.20575389\n",
      "   0.51835018  0.75718707  0.76032811]]\n",
      "\n",
      "\n",
      "[[ 0.73006338  0.27722538  0.27456337  0.20940425  0.22342221 -0.18487071\n",
      "   0.48240566  0.75363874  0.69759196]]\n",
      "[[ 0.73006338  0.27722538  0.27456337  0.20940425  0.22342221 -0.18487071\n",
      "   0.48240566  0.75363874  0.69759196]]\n",
      "[[ 0.73006338  0.27722538  0.27456337  0.20940425  0.22342221 -0.18487071\n",
      "   0.48240566  0.75363874  0.69759196]]\n",
      "[[ 0.73006338  0.27722538  0.27456337  0.20940425  0.22342221 -0.18487071\n",
      "   0.48240566  0.75363874  0.69759196]]\n",
      "\n",
      "\n",
      "[[ 0.75946665  0.26482219  0.30304703  0.22747564  0.2584236  -0.18310811\n",
      "   0.52534616  0.75953126  0.76103657]]\n",
      "[[ 0.75946665  0.26482219  0.30304703  0.22747564  0.2584236  -0.18310811\n",
      "   0.52534616  0.75953126  0.76103657]]\n",
      "[[ 0.75946665  0.26482219  0.30304703  0.22747564  0.2584236  -0.18310811\n",
      "   0.52534616  0.75953126  0.76103657]]\n",
      "[[ 0.75946665  0.26482219  0.30304703  0.22747564  0.2584236  -0.18310811\n",
      "   0.52534616  0.75953126  0.76103657]]\n",
      "[[ 0.75946665  0.26482219  0.30304703  0.22747564  0.2584236  -0.18310811\n",
      "   0.52534616  0.75953126  0.76103657]]\n",
      "\n",
      "\n",
      "[[ 0.46509275  0.23327421  0.25759161  0.19082487  0.2275093  -0.18930158\n",
      "   0.36047393  0.49542743  0.51355743]]\n",
      "[[ 0.46509275  0.23327421  0.25759161  0.19082487  0.2275093  -0.18930158\n",
      "   0.36047393  0.49542743  0.51355743]]\n",
      "[[ 0.46509275  0.23327421  0.25759161  0.19082487  0.2275093  -0.18930158\n",
      "   0.36047393  0.49542743  0.51355743]]\n",
      "[[ 0.46509275  0.23327421  0.25759161  0.19082487  0.2275093  -0.18930158\n",
      "   0.36047393  0.49542743  0.51355743]]\n",
      "[[ 0.46509275  0.23327421  0.25759161  0.19082487  0.2275093  -0.18930158\n",
      "   0.36047393  0.49542743  0.51355743]]\n",
      "\n",
      "\n",
      "[[ 0.7592591   0.28105745  0.28353608  0.23559202  0.24091755 -0.18160288\n",
      "   0.51187915  0.7529816   0.75425196]]\n",
      "[[ 0.7592591   0.28105745  0.28353608  0.23559202  0.24091755 -0.18160288\n",
      "   0.51187915  0.7529816   0.75425196]]\n",
      "[[ 0.7592591   0.28105745  0.28353608  0.23559202  0.24091755 -0.18160288\n",
      "   0.51187915  0.7529816   0.75425196]]\n",
      "[[ 0.7592591   0.28105745  0.28353608  0.23559202  0.24091755 -0.18160288\n",
      "   0.51187915  0.7529816   0.75425196]]\n",
      "[[ 0.7592591   0.28105745  0.28353608  0.23559202  0.24091755 -0.18160288\n",
      "   0.51187915  0.7529816   0.75425196]]\n",
      "[[ 0.7592591   0.28105745  0.28353608  0.23559202  0.24091755 -0.18160288\n",
      "   0.51187915  0.7529816   0.75425196]]\n",
      "[[ 0.7592591   0.28105745  0.28353608  0.23559202  0.24091755 -0.18160288\n",
      "   0.51187915  0.7529816   0.75425196]]\n",
      "[[ 0.7592591   0.28105745  0.28353608  0.23559202  0.24091755 -0.18160288\n",
      "   0.51187915  0.7529816   0.75425196]]\n",
      "[[ 0.7592591   0.28105745  0.28353608  0.23559202  0.24091755 -0.18160288\n",
      "   0.51187915  0.7529816   0.75425196]]\n",
      "[[ 0.7592591   0.28105745  0.28353608  0.23559202  0.24091755 -0.18160288\n",
      "   0.51187915  0.7529816   0.75425196]]\n",
      "[[ 0.7592591   0.28105745  0.28353608  0.23559202  0.24091755 -0.18160288\n",
      "   0.51187915  0.7529816   0.75425196]]\n",
      "\n",
      "\n",
      "[[ 0.76022726  0.26501316  0.30192065  0.22534081  0.26139665 -0.21014233\n",
      "   0.52583265  0.75822324  0.76061314]]\n",
      "[[ 0.76022726  0.26501316  0.30192065  0.22534081  0.26139665 -0.21014233\n",
      "   0.52583265  0.75822324  0.76061314]]\n",
      "\n",
      "\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "[[ 0.75917554  0.28226319  0.30306664  0.2271139   0.24587339 -0.26648474\n",
      "   0.53187704  0.7602694   0.76012725]]\n",
      "\n",
      "\n",
      "[[ 0.76037407  0.2634176   0.30262002  0.22436087  0.26145971 -0.17689189\n",
      "   0.52643275  0.75933468  0.76043051]]\n",
      "[[ 0.76037407  0.2634176   0.30262002  0.22436087  0.26145971 -0.17689189\n",
      "   0.52643275  0.75933468  0.76043051]]\n",
      "[[ 0.76037407  0.2634176   0.30262002  0.22436087  0.26145971 -0.17689189\n",
      "   0.52643275  0.75933468  0.76043051]]\n",
      "[[ 0.76037407  0.2634176   0.30262002  0.22436087  0.26145971 -0.17689189\n",
      "   0.52643275  0.75933468  0.76043051]]\n",
      "[[ 0.76037407  0.2634176   0.30262002  0.22436087  0.26145971 -0.17689189\n",
      "   0.52643275  0.75933468  0.76043051]]\n",
      "[[ 0.76037407  0.2634176   0.30262002  0.22436087  0.26145971 -0.17689189\n",
      "   0.52643275  0.75933468  0.76043051]]\n",
      "[[ 0.76037407  0.2634176   0.30262002  0.22436087  0.26145971 -0.17689189\n",
      "   0.52643275  0.75933468  0.76043051]]\n",
      "[[ 0.76037407  0.2634176   0.30262002  0.22436087  0.26145971 -0.17689189\n",
      "   0.52643275  0.75933468  0.76043051]]\n",
      "[[ 0.76037407  0.2634176   0.30262002  0.22436087  0.26145971 -0.17689189\n",
      "   0.52643275  0.75933468  0.76043051]]\n",
      "[[ 0.76037407  0.2634176   0.30262002  0.22436087  0.26145971 -0.17689189\n",
      "   0.52643275  0.75933468  0.76043051]]\n",
      "[[ 0.76037407  0.2634176   0.30262002  0.22436087  0.26145971 -0.17689189\n",
      "   0.52643275  0.75933468  0.76043051]]\n",
      "[[ 0.76037407  0.2634176   0.30262002  0.22436087  0.26145971 -0.17689189\n",
      "   0.52643275  0.75933468  0.76043051]]\n",
      "[[ 0.76037407  0.2634176   0.30262002  0.22436087  0.26145971 -0.17689189\n",
      "   0.52643275  0.75933468  0.76043051]]\n",
      "[[ 0.76037407  0.2634176   0.30262002  0.22436087  0.26145971 -0.17689189\n",
      "   0.52643275  0.75933468  0.76043051]]\n",
      "[[ 0.76037407  0.2634176   0.30262002  0.22436087  0.26145971 -0.17689189\n",
      "   0.52643275  0.75933468  0.76043051]]\n",
      "\n",
      "\n",
      "[[ 0.60050982  0.25485316  0.28447992  0.22009058  0.25374681 -0.216001\n",
      "   0.46444857  0.68663698  0.69507086]]\n",
      "[[ 0.60050982  0.25485316  0.28447992  0.22009058  0.25374681 -0.216001\n",
      "   0.46444857  0.68663698  0.69507086]]\n",
      "[[ 0.60050982  0.25485316  0.28447992  0.22009058  0.25374681 -0.216001\n",
      "   0.46444857  0.68663698  0.69507086]]\n",
      "[[ 0.60050982  0.25485316  0.28447992  0.22009058  0.25374681 -0.216001\n",
      "   0.46444857  0.68663698  0.69507086]]\n",
      "[[ 0.60050982  0.25485316  0.28447992  0.22009058  0.25374681 -0.216001\n",
      "   0.46444857  0.68663698  0.69507086]]\n",
      "[[ 0.60050982  0.25485316  0.28447992  0.22009058  0.25374681 -0.216001\n",
      "   0.46444857  0.68663698  0.69507086]]\n",
      "[[ 0.60050982  0.25485316  0.28447992  0.22009058  0.25374681 -0.216001\n",
      "   0.46444857  0.68663698  0.69507086]]\n",
      "[[ 0.60050982  0.25485316  0.28447992  0.22009058  0.25374681 -0.216001\n",
      "   0.46444857  0.68663698  0.69507086]]\n",
      "[[ 0.60050982  0.25485316  0.28447992  0.22009058  0.25374681 -0.216001\n",
      "   0.46444857  0.68663698  0.69507086]]\n",
      "[[ 0.60050982  0.25485316  0.28447992  0.22009058  0.25374681 -0.216001\n",
      "   0.46444857  0.68663698  0.69507086]]\n",
      "\n",
      "\n",
      "[[ 0.11022234  0.08731471  0.07335488  0.07286399  0.08599247 -0.08573061\n",
      "   0.12135411  0.11062302  0.12057039]]\n",
      "[[ 0.11022234  0.08731471  0.07335488  0.07286399  0.08599247 -0.08573061\n",
      "   0.12135411  0.11062302  0.12057039]]\n",
      "\n",
      "\n",
      "[[ 0.76047766  0.26276404  0.30094227  0.22503489  0.26146016 -0.17460835\n",
      "   0.51936644  0.75920707  0.76095527]]\n",
      "[[ 0.76047766  0.26276404  0.30094227  0.22503489  0.26146016 -0.17460835\n",
      "   0.51936644  0.75920707  0.76095527]]\n",
      "[[ 0.76047766  0.26276404  0.30094227  0.22503489  0.26146016 -0.17460835\n",
      "   0.51936644  0.75920707  0.76095527]]\n",
      "[[ 0.76047766  0.26276404  0.30094227  0.22503489  0.26146016 -0.17460835\n",
      "   0.51936644  0.75920707  0.76095527]]\n",
      "[[ 0.76047766  0.26276404  0.30094227  0.22503489  0.26146016 -0.17460835\n",
      "   0.51936644  0.75920707  0.76095527]]\n",
      "[[ 0.76047766  0.26276404  0.30094227  0.22503489  0.26146016 -0.17460835\n",
      "   0.51936644  0.75920707  0.76095527]]\n",
      "[[ 0.76047766  0.26276404  0.30094227  0.22503489  0.26146016 -0.17460835\n",
      "   0.51936644  0.75920707  0.76095527]]\n",
      "[[ 0.76047766  0.26276404  0.30094227  0.22503489  0.26146016 -0.17460835\n",
      "   0.51936644  0.75920707  0.76095527]]\n",
      "[[ 0.76047766  0.26276404  0.30094227  0.22503489  0.26146016 -0.17460835\n",
      "   0.51936644  0.75920707  0.76095527]]\n",
      "[[ 0.76047766  0.26276404  0.30094227  0.22503489  0.26146016 -0.17460835\n",
      "   0.51936644  0.75920707  0.76095527]]\n",
      "[[ 0.76047766  0.26276404  0.30094227  0.22503489  0.26146016 -0.17460835\n",
      "   0.51936644  0.75920707  0.76095527]]\n",
      "\n",
      "\n",
      "[[ 0.57843745  0.25027683  0.27087265  0.21323591  0.242947   -0.22263455\n",
      "   0.41774812  0.64751214  0.65812206]]\n",
      "[[ 0.57843745  0.25027683  0.27087265  0.21323591  0.242947   -0.22263455\n",
      "   0.41774812  0.64751214  0.65812206]]\n",
      "[[ 0.57843745  0.25027683  0.27087265  0.21323591  0.242947   -0.22263455\n",
      "   0.41774812  0.64751214  0.65812206]]\n",
      "[[ 0.57843745  0.25027683  0.27087265  0.21323591  0.242947   -0.22263455\n",
      "   0.41774812  0.64751214  0.65812206]]\n",
      "[[ 0.57843745  0.25027683  0.27087265  0.21323591  0.242947   -0.22263455\n",
      "   0.41774812  0.64751214  0.65812206]]\n",
      "[[ 0.57843745  0.25027683  0.27087265  0.21323591  0.242947   -0.22263455\n",
      "   0.41774812  0.64751214  0.65812206]]\n",
      "[[ 0.57843745  0.25027683  0.27087265  0.21323591  0.242947   -0.22263455\n",
      "   0.41774812  0.64751214  0.65812206]]\n",
      "[[ 0.57843745  0.25027683  0.27087265  0.21323591  0.242947   -0.22263455\n",
      "   0.41774812  0.64751214  0.65812206]]\n",
      "\n",
      "\n",
      "[[ 0.74255461  0.26386994  0.29781121  0.22415218  0.26120389 -0.18299916\n",
      "   0.51805609  0.75994313  0.76015824]]\n",
      "[[ 0.74255461  0.26386994  0.29781121  0.22415218  0.26120389 -0.18299916\n",
      "   0.51805609  0.75994313  0.76015824]]\n",
      "[[ 0.74255461  0.26386994  0.29781121  0.22415218  0.26120389 -0.18299916\n",
      "   0.51805609  0.75994313  0.76015824]]\n",
      "[[ 0.74255461  0.26386994  0.29781121  0.22415218  0.26120389 -0.18299916\n",
      "   0.51805609  0.75994313  0.76015824]]\n",
      "\n",
      "\n",
      "[[ 0.74797642  0.26538286  0.29606748  0.22037601  0.25758401 -0.21905474\n",
      "   0.52808183  0.75709742  0.76066971]]\n",
      "[[ 0.74797642  0.26538286  0.29606748  0.22037601  0.25758401 -0.21905474\n",
      "   0.52808183  0.75709742  0.76066971]]\n",
      "[[ 0.74797642  0.26538286  0.29606748  0.22037601  0.25758401 -0.21905474\n",
      "   0.52808183  0.75709742  0.76066971]]\n",
      "[[ 0.74797642  0.26538286  0.29606748  0.22037601  0.25758401 -0.21905474\n",
      "   0.52808183  0.75709742  0.76066971]]\n",
      "[[ 0.74797642  0.26538286  0.29606748  0.22037601  0.25758401 -0.21905474\n",
      "   0.52808183  0.75709742  0.76066971]]\n",
      "[[ 0.74797642  0.26538286  0.29606748  0.22037601  0.25758401 -0.21905474\n",
      "   0.52808183  0.75709742  0.76066971]]\n",
      "[[ 0.74797642  0.26538286  0.29606748  0.22037601  0.25758401 -0.21905474\n",
      "   0.52808183  0.75709742  0.76066971]]\n",
      "[[ 0.74797642  0.26538286  0.29606748  0.22037601  0.25758401 -0.21905474\n",
      "   0.52808183  0.75709742  0.76066971]]\n",
      "[[ 0.74797642  0.26538286  0.29606748  0.22037601  0.25758401 -0.21905474\n",
      "   0.52808183  0.75709742  0.76066971]]\n",
      "[[ 0.74797642  0.26538286  0.29606748  0.22037601  0.25758401 -0.21905474\n",
      "   0.52808183  0.75709742  0.76066971]]\n",
      "[[ 0.74797642  0.26538286  0.29606748  0.22037601  0.25758401 -0.21905474\n",
      "   0.52808183  0.75709742  0.76066971]]\n",
      "[[ 0.74797642  0.26538286  0.29606748  0.22037601  0.25758401 -0.21905474\n",
      "   0.52808183  0.75709742  0.76066971]]\n",
      "\n",
      "\n",
      "[[ 0.75957417  0.2647242   0.29761505  0.22453424  0.25513121 -0.18144225\n",
      "   0.5242821   0.75772917  0.76059777]]\n",
      "[[ 0.75957417  0.2647242   0.29761505  0.22453424  0.25513121 -0.18144225\n",
      "   0.5242821   0.75772917  0.76059777]]\n",
      "[[ 0.75957417  0.2647242   0.29761505  0.22453424  0.25513121 -0.18144225\n",
      "   0.5242821   0.75772917  0.76059777]]\n",
      "[[ 0.75957417  0.2647242   0.29761505  0.22453424  0.25513121 -0.18144225\n",
      "   0.5242821   0.75772917  0.76059777]]\n",
      "[[ 0.75957417  0.2647242   0.29761505  0.22453424  0.25513121 -0.18144225\n",
      "   0.5242821   0.75772917  0.76059777]]\n",
      "[[ 0.75957417  0.2647242   0.29761505  0.22453424  0.25513121 -0.18144225\n",
      "   0.5242821   0.75772917  0.76059777]]\n",
      "[[ 0.75957417  0.2647242   0.29761505  0.22453424  0.25513121 -0.18144225\n",
      "   0.5242821   0.75772917  0.76059777]]\n",
      "[[ 0.75957417  0.2647242   0.29761505  0.22453424  0.25513121 -0.18144225\n",
      "   0.5242821   0.75772917  0.76059777]]\n",
      "[[ 0.75957417  0.2647242   0.29761505  0.22453424  0.25513121 -0.18144225\n",
      "   0.5242821   0.75772917  0.76059777]]\n",
      "[[ 0.75957417  0.2647242   0.29761505  0.22453424  0.25513121 -0.18144225\n",
      "   0.5242821   0.75772917  0.76059777]]\n",
      "[[ 0.75957417  0.2647242   0.29761505  0.22453424  0.25513121 -0.18144225\n",
      "   0.5242821   0.75772917  0.76059777]]\n",
      "[[ 0.75957417  0.2647242   0.29761505  0.22453424  0.25513121 -0.18144225\n",
      "   0.5242821   0.75772917  0.76059777]]\n",
      "[[ 0.75957417  0.2647242   0.29761505  0.22453424  0.25513121 -0.18144225\n",
      "   0.5242821   0.75772917  0.76059777]]\n",
      "[[ 0.75957417  0.2647242   0.29761505  0.22453424  0.25513121 -0.18144225\n",
      "   0.5242821   0.75772917  0.76059777]]\n",
      "[[ 0.75957417  0.2647242   0.29761505  0.22453424  0.25513121 -0.18144225\n",
      "   0.5242821   0.75772917  0.76059777]]\n",
      "[[ 0.75957417  0.2647242   0.29761505  0.22453424  0.25513121 -0.18144225\n",
      "   0.5242821   0.75772917  0.76059777]]\n",
      "[[ 0.75957417  0.2647242   0.29761505  0.22453424  0.25513121 -0.18144225\n",
      "   0.5242821   0.75772917  0.76059777]]\n",
      "[[ 0.75957417  0.2647242   0.29761505  0.22453424  0.25513121 -0.18144225\n",
      "   0.5242821   0.75772917  0.76059777]]\n",
      "[[ 0.75957417  0.2647242   0.29761505  0.22453424  0.25513121 -0.18144225\n",
      "   0.5242821   0.75772917  0.76059777]]\n",
      "[[ 0.75957417  0.2647242   0.29761505  0.22453424  0.25513121 -0.18144225\n",
      "   0.5242821   0.75772917  0.76059777]]\n",
      "[[ 0.75957417  0.2647242   0.29761505  0.22453424  0.25513121 -0.18144225\n",
      "   0.5242821   0.75772917  0.76059777]]\n",
      "[[ 0.75957417  0.2647242   0.29761505  0.22453424  0.25513121 -0.18144225\n",
      "   0.5242821   0.75772917  0.76059777]]\n",
      "[[ 0.75957417  0.2647242   0.29761505  0.22453424  0.25513121 -0.18144225\n",
      "   0.5242821   0.75772917  0.76059777]]\n",
      "\n",
      "\n",
      "[[ 0.7475782   0.26412505  0.29958388  0.22807163  0.25622338 -0.24373235\n",
      "   0.52449071  0.75968659  0.76019382]]\n",
      "[[ 0.7475782   0.26412505  0.29958388  0.22807163  0.25622338 -0.24373235\n",
      "   0.52449071  0.75968659  0.76019382]]\n",
      "[[ 0.7475782   0.26412505  0.29958388  0.22807163  0.25622338 -0.24373235\n",
      "   0.52449071  0.75968659  0.76019382]]\n",
      "[[ 0.7475782   0.26412505  0.29958388  0.22807163  0.25622338 -0.24373235\n",
      "   0.52449071  0.75968659  0.76019382]]\n",
      "[[ 0.7475782   0.26412505  0.29958388  0.22807163  0.25622338 -0.24373235\n",
      "   0.52449071  0.75968659  0.76019382]]\n",
      "[[ 0.7475782   0.26412505  0.29958388  0.22807163  0.25622338 -0.24373235\n",
      "   0.52449071  0.75968659  0.76019382]]\n",
      "[[ 0.7475782   0.26412505  0.29958388  0.22807163  0.25622338 -0.24373235\n",
      "   0.52449071  0.75968659  0.76019382]]\n",
      "[[ 0.7475782   0.26412505  0.29958388  0.22807163  0.25622338 -0.24373235\n",
      "   0.52449071  0.75968659  0.76019382]]\n",
      "[[ 0.7475782   0.26412505  0.29958388  0.22807163  0.25622338 -0.24373235\n",
      "   0.52449071  0.75968659  0.76019382]]\n",
      "[[ 0.7475782   0.26412505  0.29958388  0.22807163  0.25622338 -0.24373235\n",
      "   0.52449071  0.75968659  0.76019382]]\n",
      "[[ 0.7475782   0.26412505  0.29958388  0.22807163  0.25622338 -0.24373235\n",
      "   0.52449071  0.75968659  0.76019382]]\n",
      "[[ 0.7475782   0.26412505  0.29958388  0.22807163  0.25622338 -0.24373235\n",
      "   0.52449071  0.75968659  0.76019382]]\n",
      "[[ 0.7475782   0.26412505  0.29958388  0.22807163  0.25622338 -0.24373235\n",
      "   0.52449071  0.75968659  0.76019382]]\n",
      "[[ 0.7475782   0.26412505  0.29958388  0.22807163  0.25622338 -0.24373235\n",
      "   0.52449071  0.75968659  0.76019382]]\n",
      "[[ 0.7475782   0.26412505  0.29958388  0.22807163  0.25622338 -0.24373235\n",
      "   0.52449071  0.75968659  0.76019382]]\n",
      "[[ 0.7475782   0.26412505  0.29958388  0.22807163  0.25622338 -0.24373235\n",
      "   0.52449071  0.75968659  0.76019382]]\n",
      "\n",
      "\n",
      "[[ 0.75768787  0.26341534  0.29789624  0.22702399  0.25802621 -0.1979983\n",
      "   0.52137089  0.76001966  0.75780201]]\n",
      "[[ 0.75768787  0.26341534  0.29789624  0.22702399  0.25802621 -0.1979983\n",
      "   0.52137089  0.76001966  0.75780201]]\n",
      "[[ 0.75768787  0.26341534  0.29789624  0.22702399  0.25802621 -0.1979983\n",
      "   0.52137089  0.76001966  0.75780201]]\n",
      "[[ 0.75768787  0.26341534  0.29789624  0.22702399  0.25802621 -0.1979983\n",
      "   0.52137089  0.76001966  0.75780201]]\n",
      "[[ 0.75768787  0.26341534  0.29789624  0.22702399  0.25802621 -0.1979983\n",
      "   0.52137089  0.76001966  0.75780201]]\n",
      "[[ 0.75768787  0.26341534  0.29789624  0.22702399  0.25802621 -0.1979983\n",
      "   0.52137089  0.76001966  0.75780201]]\n",
      "[[ 0.75768787  0.26341534  0.29789624  0.22702399  0.25802621 -0.1979983\n",
      "   0.52137089  0.76001966  0.75780201]]\n",
      "[[ 0.75768787  0.26341534  0.29789624  0.22702399  0.25802621 -0.1979983\n",
      "   0.52137089  0.76001966  0.75780201]]\n",
      "[[ 0.75768787  0.26341534  0.29789624  0.22702399  0.25802621 -0.1979983\n",
      "   0.52137089  0.76001966  0.75780201]]\n",
      "\n",
      "\n",
      "[[ 0.76083565  0.26551971  0.30811122  0.22819674  0.26598838 -0.2304876\n",
      "   0.5309822   0.760975    0.76121604]]\n",
      "[[ 0.76083565  0.26551971  0.30811122  0.22819674  0.26598838 -0.2304876\n",
      "   0.5309822   0.760975    0.76121604]]\n",
      "[[ 0.76083565  0.26551971  0.30811122  0.22819674  0.26598838 -0.2304876\n",
      "   0.5309822   0.760975    0.76121604]]\n",
      "[[ 0.76083565  0.26551971  0.30811122  0.22819674  0.26598838 -0.2304876\n",
      "   0.5309822   0.760975    0.76121604]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.76083565  0.26551971  0.30811122  0.22819674  0.26598838 -0.2304876\n",
      "   0.5309822   0.760975    0.76121604]]\n",
      "\n",
      "\n",
      "252 261875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2]), array([223,   2,  27]))"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poses = []\n",
    "\n",
    "model = models[2]\n",
    "\n",
    "features = np.nan_to_num(X_combined[-sum(l_chunks[-1]):])\n",
    "scaler = preprocessing.MinMaxScaler().fit(features)\n",
    "features = scaler.transform(features)\n",
    "poses = []\n",
    "idx_new=0\n",
    "counter=-1\n",
    "#find closest audio vec in dataset instead of assuming you have in dataset, remove from og dataset what used etc, precaution etc\n",
    "# should ideally do by sample frames based on dataset stats but need timings to match so using duration\n",
    "#approach\n",
    "for idx,k in enumerate(audio_portions_dicts[-1].keys()):\n",
    "    v=audio_portions_dicts[-1][k]\n",
    "    if v==1:\n",
    "        counter+=1\n",
    "        \n",
    "        #print(output_frame_angles)\n",
    "        #rescale output?\n",
    "        #scaler = preprocessing.MinMaxScaler().fit(output_frame_angles)\n",
    "        #output_frame_angles = scaler.transform(output_frame_angles)\n",
    "        #print(output_frame_angles)\n",
    "        \n",
    "        #same prosody same output hmm\n",
    "        #if testing and chunking by prosody then doesnt work for emotion hmm\n",
    "        # for testing, split audio by seconds, more for emotion obviously, get more variation in pose...\n",
    "        #to make work for current timing idea etc\n",
    "        for i in range(round((k[1]-k[0])/1000)):\n",
    "            output_frame_angles = model.predict(features[np.newaxis,idx_new:idx_new+1,:]) #train every time hmm\n",
    "            print(output_frame_angles)\n",
    "            dists_output_to_base_poses = np.sqrt(np.sum(np.square(output_frame_angles - angles),axis=1))\n",
    "            output_pose_id = np.argmin(dists_output_to_base_poses)\n",
    "            poses.append(output_pose_id)\n",
    "        print('\\n')\n",
    "        idx_new += l_chunks[-1][counter]\n",
    "    else:\n",
    "        poses.append(2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #scale and squash dim!\n",
    "print(len(poses), l_audio_chunks[-1])\n",
    "\n",
    "with open('poses_prosody+emotion_'+audio_src+'.txt', 'w') as f:\n",
    "    for pose in poses:\n",
    "        f.write(\"%s\\n\" % pose)\n",
    "        \n",
    "np.unique(poses,return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emotion\n",
    "statement_1_em = np.mean([2, 2, 2])\n",
    "statement_2_em = np.mean([4, 3, 2])\n",
    "statement_3_em = np.mean([2, 2, 1])\n",
    "statement_4_em = np.mean([3, 2, 3])\n",
    "statement_5_em = np.mean([2, 4, 4])\n",
    "statement_6_em = np.mean([3, 2, 3])\n",
    "statement_7_em = np.mean([2, 2, 2])\n",
    "statement_8_em = np.mean([2, 2, 3])\n",
    "\n",
    "statement_1_em1 = np.std([2, 2, 2])\n",
    "statement_2_em1 = np.std([4, 3, 2])\n",
    "statement_3_em1 = np.std([2, 2, 1])\n",
    "statement_4_em1 = np.std([3, 2, 3])\n",
    "statement_5_em1 = np.std([2, 4, 4])\n",
    "statement_6_em1 = np.std([3, 2, 3])\n",
    "statement_7_em1 = np.std([2, 2, 2])\n",
    "statement_8_em1 = np.std([2, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# human playback\n",
    "statement_1_human = np.mean([3, 5, 3, 2])\n",
    "statement_2_human = np.mean([3, 5, 3, 1])\n",
    "statement_3_human = np.mean([3, 5, 1, 1])\n",
    "statement_4_human = np.mean([3, 4, 5, 1])\n",
    "statement_5_human = np.mean([4, 5, 3, 2])\n",
    "statement_6_human = np.mean([4, 5, 5, 1])\n",
    "statement_7_human = np.mean([4, 5, 1, 1])\n",
    "statement_8_human = np.mean([5, 4, 1, 1])\n",
    "\n",
    "statement_1_human1 = np.std([3, 5, 3, 2])\n",
    "statement_2_human1 = np.std([3, 5, 3, 1])\n",
    "statement_3_human1 = np.std([3, 5, 1, 1])\n",
    "statement_4_human1 = np.std([3, 4, 5, 1])\n",
    "statement_5_human1 = np.std([4, 5, 3, 2])\n",
    "statement_6_human1 = np.std([4, 5, 5, 1])\n",
    "statement_7_human1 = np.std([4, 5, 1, 1])\n",
    "statement_8_human1 = np.std([5, 4, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEWCAYAAABliCz2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1413575c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "human = [statement_1_human, statement_2_human, statement_3_human, statement_4_human, statement_5_human, statement_6_human, statement_7_human, statement_8_human]\n",
    "em = [statement_1_em, statement_2_em, statement_3_em, statement_4_em, statement_5_em, statement_6_em, statement_7_em, statement_8_em]\n",
    "\n",
    "err_human = [statement_1_human1, statement_2_human1, statement_3_human1, statement_4_human1, statement_5_human, statement_6_human1, statement_7_human1, statement_8_human1]\n",
    "err_em = [statement_1_em1, statement_2_em1, statement_3_em1, statement_4_em1, statement_5_em, statement_6_em1, statement_7_em1, statement_8_em1]\n",
    "\n",
    "width = 0.15\n",
    "offset = 0.01\n",
    "plt.bar(np.array(range(8))-0.8*width+offset, human, width, label='human playback', color=['lightblue']*8, edgecolor=['black']*8, yerr=err_human)\n",
    "plt.bar(np.array(range(8))+0.8*width+offset, em, width, label='emotion-based synthesis', color=['orange']*8, edgecolor=['black']*8, yerr=err_em)\n",
    "\n",
    "\n",
    "#plt.bar(np.array(range(3))+2*width-offset, logistic_noshuf, width, label='logistic regression, not shuffled', hatch='//', color=['orange']*3, edgecolor=['black']*3)\n",
    "plt.legend(bbox_to_anchor=(0.5, 0.95))\n",
    "plt.xticks(range(8), ['1', '2', '3', '4', '5', '6', '7', '8'])\n",
    "plt.ylabel('mean 5-point likert scale score')\n",
    "plt.xlabel('survey statement index')\n",
    "plt.title('survey summary (2 experimental conditions)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
