{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio  # for reading in video files\n",
    "\n",
    "# Imports the Google Cloud client library\n",
    "from google.cloud import speech\n",
    "from google.cloud.speech import enums\n",
    "from google.cloud.speech import types\n",
    "\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from __future__ import print_function, division, unicode_literals\n",
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "\n",
    "sys.path.insert(0, \"/Users/anuj/coursework_cuboulder/spring_2018/algo_hri/torchMoji-master\")\n",
    "\n",
    "from torchmoji.sentence_tokenizer import SentenceTokenizer\n",
    "from torchmoji.model_def import torchmoji_emojis\n",
    "from torchmoji.global_variables import PRETRAINED_PATH, VOCAB_PATH\n",
    "\n",
    "import subprocess\n",
    "\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "from pydub.silence import detect_nonsilent\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_audio_to_mono(data_folder):\n",
    "    output_filename = data_folder+'mono_myrecording.wav'\n",
    "    command = ['sox', data_folder+'myrecording.wav', '-c', '1', output_filename] \n",
    "    subprocess.Popen(command)\n",
    "    \n",
    "    print('mono audio saved to', output_filename)\n",
    "    return output_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relative_joint_positions(data_folder):\n",
    "    def get_skeleton_dict(data_folder, real_world_coords=True):\n",
    "        if real_world_coords:\n",
    "            filename = data_folder+'realWorldCoords.json'\n",
    "        else:\n",
    "            filename = data_folder+'projSpaceCoords.json'\n",
    "        json_contents = json.load(open(filename))\n",
    "\n",
    "        return json_contents\n",
    "\n",
    "    skeletal_data = get_skeleton_dict(data_folder, True)\n",
    "    output = []\n",
    "    for frame in skeletal_data:\n",
    "        d = {}\n",
    "        x_orig_torso, y_orig_torso, z_orig_torso = frame['torso'].split(',')\n",
    "        x_orig_torso, y_orig_torso, z_orig_torso = float(x_orig_torso[1:]), float(y_orig_torso), float(z_orig_torso[:-1])\n",
    "        for joint, position in frame.items():\n",
    "            if joint == 'time_ms':\n",
    "                d[joint] = position\n",
    "                continue\n",
    "            if joint not in d.keys():\n",
    "                d[joint] = []\n",
    "            x_orig, y_orig, z_orig = position.split(',')\n",
    "            x_orig, y_orig, z_orig = float(x_orig[1:]), float(y_orig), float(z_orig[:-1])\n",
    "            d[joint] = [x_orig - x_orig_torso, y_orig - y_orig_torso, z_orig - z_orig_torso]\n",
    "        output.append(d)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prosody_features_from_audio(audio_filename):\n",
    "    #audio_filename = convert_audio_to_mono(data_folder)\n",
    "    name = audio_filename.split('/')[-1].split('.wav')[0]\n",
    "    command = 'python /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/DisVoice-master/prosody/prosody.py \\\"'+audio_filename+'\\\" \\\"'+'/'.join(audio_filename.split('/')[:-1])+'/'+'prosody_'+ name+'.txt\\\" \"static\" \"false\"'\n",
    "    subprocess.call(command, shell=True)\n",
    "    print('prosody features saved to '+command.split('\"')[-6])\n",
    "    return command.split('\"')[-6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_speech(audio_filename):\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"google_creds.json\"\n",
    "\n",
    "    # Instantiates a client\n",
    "    client = speech.SpeechClient()\n",
    "\n",
    "    # Loads the audio into memory\n",
    "    with io.open(audio_filename, 'rb') as audio_file:\n",
    "        content = audio_file.read()\n",
    "        audio = types.RecognitionAudio(content=content)\n",
    "\n",
    "    config = types.RecognitionConfig(language_code='en-US')\n",
    "\n",
    "    # Detects speech in the audio file\n",
    "    text = ''\n",
    "    response = client.recognize(config, audio)\n",
    "    if len(response.results) != 0:\n",
    "        text = response.results[0].alternatives[0].transcript\n",
    "    a = audio_filename.split('/')\n",
    "    filename = '/'+'/'.join(a[1:-1])+'/text_'+a[-1].split('.wav')[0]+'.txt'\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(text)\n",
    "    print('converted text saved to '+filename)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emotion_features_from_text(text, audio_filename):\n",
    "    if text == '':\n",
    "        emoji_ids = []\n",
    "        one_hot_encodings = []\n",
    "    else:\n",
    "        text = [text]\n",
    "        def top_elements(array, k):\n",
    "            ind = np.argpartition(array, -k)[-k:]\n",
    "            return ind[np.argsort(array[ind])][::-1]\n",
    "\n",
    "        maxlen = 30\n",
    "\n",
    "        #print('Tokenizing using dictionary from {}'.format(VOCAB_PATH))\n",
    "        with open(VOCAB_PATH, 'r') as f:\n",
    "            vocabulary = json.load(f)\n",
    "\n",
    "        st = SentenceTokenizer(vocabulary, maxlen)\n",
    "\n",
    "        #print('Loading model from {}.'.format(PRETRAINED_PATH))\n",
    "        model = torchmoji_emojis(PRETRAINED_PATH)\n",
    "        #print(model)\n",
    "        #print('Running predictions.')\n",
    "        tokenized, _, _ = st.tokenize_sentences(text)\n",
    "        prob = model(tokenized)\n",
    "\n",
    "        for prob in [prob]:\n",
    "            # Find top emojis for each sentence. Emoji ids (0-63)\n",
    "            # correspond to the mapping in emoji_overview.png\n",
    "            # at the root of the torchMoji repo.\n",
    "            #print('Writing results to {}'.format(OUTPUT_PATH))\n",
    "            scores = []\n",
    "            for i, t in enumerate(text):\n",
    "                t_tokens = tokenized[i]\n",
    "                t_score = [t]\n",
    "                t_prob = prob[i]\n",
    "                ind_top = top_elements(t_prob, 5)\n",
    "                t_score.append(sum(t_prob[ind_top]))\n",
    "                t_score.extend(ind_top)\n",
    "                t_score.extend([t_prob[ind] for ind in ind_top])\n",
    "                scores.append(t_score)\n",
    "\n",
    "        emoji_ids = scores[0][2:2+5]\n",
    "        one_hot_encodings = []\n",
    "        for emoji_idx in emoji_ids:\n",
    "            one_hot_encodings.append([0 if i!=emoji_idx else 1 for i in range(64)])\n",
    "    a = audio_filename.split('/')\n",
    "    filename = '/'+'/'.join(a[1:-1])+'/onehot_emotion_'+a[-1].split('.wav')[0]+'.pkl'\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(one_hot_encodings, f)\n",
    "        \n",
    "    print('one hot encoded emotion vector saved to '+filename)\n",
    "    filename = '/'+'/'.join(a[1:-1])+'/emoji_ids_'+a[-1].split('.wav')[0]+'.pkl'\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(emoji_ids, f)\n",
    "    print('emoji ids saved to '+filename)\n",
    "    return emoji_ids, one_hot_encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mono audio saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/mono_myrecording.wav\n",
      "#############\n",
      "exporting /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/chunk0.wav\n",
      "prosody features saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/prosody_chunk0.txt\n",
      "converted text saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/text_chunk0.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anuj/anaconda/lib/python3.6/site-packages/torch/nn/modules/container.py:67: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one hot encoded emotion vector saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/onehot_emotion_chunk0.pkl\n",
      "emoji ids saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/emoji_ids_chunk0.pkl\n",
      "#############\n",
      "exporting /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/chunk1.wav\n",
      "prosody features saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/prosody_chunk1.txt\n",
      "converted text saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/text_chunk1.txt\n",
      "one hot encoded emotion vector saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/onehot_emotion_chunk1.pkl\n",
      "emoji ids saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/emoji_ids_chunk1.pkl\n",
      "#############\n",
      "exporting /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/chunk2.wav\n",
      "prosody features saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/prosody_chunk2.txt\n",
      "converted text saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/text_chunk2.txt\n",
      "one hot encoded emotion vector saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/onehot_emotion_chunk2.pkl\n",
      "emoji ids saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/emoji_ids_chunk2.pkl\n",
      "#############\n",
      "exporting /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/chunk3.wav\n",
      "prosody features saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/prosody_chunk3.txt\n",
      "converted text saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/text_chunk3.txt\n",
      "one hot encoded emotion vector saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/onehot_emotion_chunk3.pkl\n",
      "emoji ids saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/emoji_ids_chunk3.pkl\n",
      "#############\n",
      "exporting /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/chunk4.wav\n",
      "prosody features saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/prosody_chunk4.txt\n",
      "converted text saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/text_chunk4.txt\n",
      "one hot encoded emotion vector saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/onehot_emotion_chunk4.pkl\n",
      "emoji ids saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/emoji_ids_chunk4.pkl\n",
      "#############\n",
      "exporting /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/chunk5.wav\n",
      "prosody features saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/prosody_chunk5.txt\n",
      "converted text saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/text_chunk5.txt\n",
      "one hot encoded emotion vector saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/onehot_emotion_chunk5.pkl\n",
      "emoji ids saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/emoji_ids_chunk5.pkl\n",
      "#############\n",
      "exporting /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/chunk6.wav\n",
      "prosody features saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/prosody_chunk6.txt\n",
      "converted text saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/text_chunk6.txt\n",
      "one hot encoded emotion vector saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/onehot_emotion_chunk6.pkl\n",
      "emoji ids saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/emoji_ids_chunk6.pkl\n",
      "#############\n",
      "exporting /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/chunk7.wav\n",
      "prosody features saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/prosody_chunk7.txt\n",
      "converted text saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/text_chunk7.txt\n",
      "one hot encoded emotion vector saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/onehot_emotion_chunk7.pkl\n",
      "emoji ids saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/emoji_ids_chunk7.pkl\n",
      "#############\n",
      "exporting /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/chunk8.wav\n",
      "prosody features saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/prosody_chunk8.txt\n",
      "converted text saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/text_chunk8.txt\n",
      "one hot encoded emotion vector saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/onehot_emotion_chunk8.pkl\n",
      "emoji ids saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/emoji_ids_chunk8.pkl\n",
      "#############\n",
      "exporting /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/chunk9.wav\n",
      "prosody features saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/prosody_chunk9.txt\n",
      "converted text saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/text_chunk9.txt\n",
      "one hot encoded emotion vector saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/onehot_emotion_chunk9.pkl\n",
      "emoji ids saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/emoji_ids_chunk9.pkl\n",
      "#############\n",
      "exporting /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/chunk10.wav\n",
      "prosody features saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/prosody_chunk10.txt\n",
      "converted text saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/text_chunk10.txt\n",
      "one hot encoded emotion vector saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/onehot_emotion_chunk10.pkl\n",
      "emoji ids saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/emoji_ids_chunk10.pkl\n",
      "#############\n",
      "exporting /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/chunk11.wav\n",
      "prosody features saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/prosody_chunk11.txt\n",
      "converted text saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/text_chunk11.txt\n",
      "one hot encoded emotion vector saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/onehot_emotion_chunk11.pkl\n",
      "emoji ids saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/emoji_ids_chunk11.pkl\n",
      "#############\n",
      "exporting /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/chunk12.wav\n",
      "prosody features saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/prosody_chunk12.txt\n",
      "converted text saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/text_chunk12.txt\n",
      "one hot encoded emotion vector saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/onehot_emotion_chunk12.pkl\n",
      "emoji ids saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/emoji_ids_chunk12.pkl\n",
      "#############\n",
      "exporting /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/chunk13.wav\n",
      "prosody features saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/prosody_chunk13.txt\n",
      "converted text saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/text_chunk13.txt\n",
      "one hot encoded emotion vector saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/onehot_emotion_chunk13.pkl\n",
      "emoji ids saved to /Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/chunks/emoji_ids_chunk13.pkl\n"
     ]
    }
   ],
   "source": [
    "data_folder = '/Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/'\n",
    "\n",
    "mono_audio_filename = convert_audio_to_mono(data_folder)\n",
    "\n",
    "###### get skeleton frames for each non-silent audio portion ######\n",
    "\n",
    "skeleton_dict = get_relative_joint_positions(data_folder)\n",
    "\n",
    "audio_portions = detect_nonsilent(sound_file, min_silence_len=500, silence_thresh=-50, seek_step=1)\n",
    "skeleton_frames = []\n",
    "for pair in audio_portions:\n",
    "    start = pair[0]\n",
    "    end = pair[1]\n",
    "    fr = []\n",
    "    for d in skeleton_dict:\n",
    "        if d['time_ms'] >= start and d['time_ms'] <= end:\n",
    "            d_tmp = d.copy()\n",
    "            del d_tmp['time_ms']\n",
    "            fr.append(d_tmp)\n",
    "    skeleton_frames.append(fr)\n",
    "\n",
    "with open(data_folder+'relative_joint_positions.json', 'w') as f:\n",
    "    json.dump(skeleton_frames, f)\n",
    "    \n",
    "sound_file = AudioSegment.from_wav(mono_audio_filename)\n",
    "audio_chunks = split_on_silence(sound_file, \n",
    "    # must be silent for at least half a second\n",
    "    min_silence_len=500,\n",
    "\n",
    "    # consider it silent if quieter than -16 dBFS\n",
    "    silence_thresh=-50\n",
    ")\n",
    "\n",
    "###### get feature vectors for each non-silent audio portion #######\n",
    "\n",
    "all_prosody_vectors = []\n",
    "all_emoji_ids = []  # not used\n",
    "all_emotion_vectors = []\n",
    "\n",
    "if not os.path.isdir(data_folder+'chunks/'):\n",
    "    os.mkdir(data_folder+'chunks/')\n",
    "\n",
    "for i, chunk in enumerate(audio_chunks):\n",
    "    print('#############')\n",
    "    out_file = data_folder+\"chunks/chunk{0}.wav\".format(i)\n",
    "    print(\"exporting\", out_file)\n",
    "    chunk.export(out_file, format=\"wav\")\n",
    "\n",
    "    ###### prosody features #######\n",
    "    \n",
    "    prosody_features_filename = get_prosody_features_from_audio(out_file)\n",
    "    with open(prosody_features_filename, 'r') as f:\n",
    "        prosody_features = list(map(float, f.read()[:-1].split(' ')))\n",
    "    all_prosody_vectors.append(prosody_features)\n",
    "    \n",
    "    ###### emotion features #######\n",
    "\n",
    "    converted_text = get_text_from_speech(out_file)\n",
    "    emoji_ids, emotion_features = get_emotion_features_from_text(converted_text, out_file)\n",
    "    all_emoji_ids.append(emoji_ids)\n",
    "    all_emotion_vectors.append(emotion_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 14, 14)"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_prosody_vectors), len(all_emotion_vectors), len(all_emoji_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JUNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_video_file(filename):\n",
    "    vid = imageio.get_reader(filename, 'ffmpeg')\n",
    "    return vid\n",
    "\n",
    "def extract_audio_from_video(video_filename):\n",
    "    video_filename = video_filename.split(' ')\n",
    "    video_filename = '\\ '.join(video_filename)\n",
    "    \n",
    "    rmp = video_filename.split('.mp4')\n",
    "    rmp.append('.wav')\n",
    "    output_audio_filename = ''.join(rmp)\n",
    "        \n",
    "    command = \"ffmpeg -i \"+video_filename+\" -ab 160k -ac 2 -ar 44100 -vn \"+output_audio_filename\n",
    "    subprocess.call(command, shell=True)\n",
    "    \n",
    "    print('audio saved to', output_audio_filename)\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "def get_skeleton_frames_consolidated_dict(directory):\n",
    "    # key = frame index\n",
    "    # value = parts position dictionary (part number: [x,y,z,c] -- pixel coordinates)\n",
    "    skeleton_frames_dict = {}\n",
    "\n",
    "    # TODO: why are some part positions empty?\n",
    "    # TODO: get 3D coordinates intead of 2D\n",
    "    for idx, skeleton_frame_filename in enumerate(sorted(os.listdir(directory))):\n",
    "        json_contents = json.load(open(directory + skeleton_frame_filename))\n",
    "        part_pos_dict = json_contents['part_candidates'][0]\n",
    "        if idx not in skeleton_frames_dict.keys():\n",
    "            skeleton_frames_dict[idx] = {}\n",
    "        skeleton_frames_dict[idx] = part_pos_dict\n",
    "        \n",
    "    return skeleton_frames_dict\n",
    "\n",
    "\"\"\"\n",
    "Check if number of frames extracted from original video == number of frames from OpenPose\n",
    "\"\"\"\n",
    "\n",
    "video_filename = 'WhatsApp Video 2018-04-12 at 10.16.35 PM.mp4'\n",
    "skeleton_json_folder = 'json/'\n",
    "\n",
    "vid = read_video_file(video_filename)\n",
    "\n",
    "skeleton_frames_dict = get_skeleton_frames_consolidated_dict(skeleton_json_folder)\n",
    "\n",
    "vid.get_meta_data()['nframes'], len(skeleton_frames_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample from recording? to get some audio?\n",
    "# frame by frame speech to text convert...\n",
    "#assume you have perfect text, how will you segment frames by sentences...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ms(s, frame_rate):\n",
    "    return s#(s/1000)*frame_rate\n",
    "\n",
    "ls = []\n",
    "for pair in audio_portions:\n",
    "    start = pair[0]\n",
    "    end = pair[1]\n",
    "    \n",
    "    ls.append((int(get_ms(start, sound_file.frame_rate)), int(get_ms(end, sound_file.frame_rate))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_gcs(gcs_uri):\n",
    "    \"\"\"Asynchronously transcribes the audio file specified by the gcs_uri.\"\"\"\n",
    "    from google.cloud import speech\n",
    "    from google.cloud.speech import enums\n",
    "    from google.cloud.speech import types\n",
    "\n",
    "    client = speech.SpeechClient()\n",
    "\n",
    "    audio = types.RecognitionAudio(uri=gcs_uri)\n",
    "    config = types.RecognitionConfig(\n",
    "        language_code='en-US')\n",
    "\n",
    "    operation = client.long_running_recognize(config, audio)\n",
    "\n",
    "    print('Waiting for operation to complete...')\n",
    "    response = operation.result()\n",
    "    \n",
    "    return response.results[0].alternatives[0].transcript\n",
    "\n",
    "    # Each result is for a consecutive portion of the audio. Iterate through\n",
    "    # them to get the transcripts for the entire audio file.\n",
    "    #for result in response.results:\n",
    "        # The first alternative is the most likely one for this portion.\n",
    "    #    print(u'Transcript: {}'.format(result.alternatives[0].transcript))\n",
    "    #    print('Confidence: {}'.format(result.alternatives[0].confidence))\n",
    "transcribe_gcs('gs://tttanuj/mono_myrecording.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first segment audio from text?\n",
    "# find pauses instead of ends of sentences?\n",
    "\n",
    "len(ls)\n",
    "#len(sound_file) is in milliseconds, frame_rate is in hertz\n",
    "\n",
    "from scipy.io import wavfile\n",
    "audio = wavfile.read('/Users/anuj/coursework_cuboulder/spring_2018/algo_hri/data/data3/mono_myrecording.wav')\n",
    "audio[1].shape\n",
    "\n",
    "#sampling 44100 times per second, 95 seconds\n",
    "\n",
    "prev_time_idx = 0\n",
    "snippets = []\n",
    "for idx,d in enumerate(skeletal_data):\n",
    "    curr_time_idx = int(audio[0]*(d['time_ms']/1000))\n",
    "    snippet = audio[1][prev_time_idx:curr_time_idx]\n",
    "    prev_time_idx = curr_time_idx\n",
    "    wavfile.write(data_folder+'audio/'+str(idx)+'.wav', audio[0], snippet)\n",
    "    snippets.append(snippet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "for pair in audio_portions:\n",
    "    start = pair[0]\n",
    "    end = pair[1]\n",
    "    fr = []\n",
    "    for d in sk:\n",
    "        if d['time_ms'] >= start and d['time_ms'] <= end:\n",
    "            fr.append(d)\n",
    "    frames.append(fr)\n",
    "\n",
    "s=0\n",
    "for x in frames:\n",
    "    s+=len(x)\n",
    "    print(len(x),s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
