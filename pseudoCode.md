Prosody Extraction Module - Given an audio input, it gives the prosody features for each frame.

// python audioAnalysis.py featureExtractionFile -i data/speech_music_sample.wav -mw 1.0 -ms 1.0 -sw 0.050 -ss 0.050 -o data/speech_music_sample.wav
// Gives the output in a csv file.
// Each value set corresponds to one time frame.


Mapper module - Takes input coordinates from OpenPose and maps the positions to NAO Robot.

// Read the stream of outputs generated by OpenPose. (It contains the 3D output)
// One set of value corresponds to one frame.
// For each set of values, normalize them to a a certain range.
// Create a vector from one joint to the another and then convert those values to angles. (Trial and error)
// Use the normalized values to build the joint positions of NAO.
// Do some interpolation to make the movements smoother.
// Feed the mapped values to the NAO Robot.


Generator Module - Once the model is trained with audio and gesture output, this module gives a corresponding gesture output values given a audio file.
It divides the audio into frames and gives the corresponding gesture values in x, y z coordinates for each frame.

// Input the trained HMM model, gestures are the hidden states and audio is the observation here.
// Given the observation i.e audio, This module gives the corresponding gestures for each frame.
// The gesture output that this module gives similar to the OpenPose output JSON file. (https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/output.md)
// The mapper module uses this gesture JSON files and uses the mapper module to convert the human joint positions to that of the NAO robot.
// The joint positions are then fed into the NAO to play those gestures.



